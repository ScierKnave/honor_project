{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJYw8wfzOBhA",
        "outputId": "e52be978-2ba4-4366-e31b-fc1e39f5621d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ScierKnave/TorchMPS.git\n",
            "  Cloning https://github.com/ScierKnave/TorchMPS.git to /tmp/pip-req-build-saqm0lz_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ScierKnave/TorchMPS.git /tmp/pip-req-build-saqm0lz_\n",
            "  Resolved https://github.com/ScierKnave/TorchMPS.git to commit f716a08e15d0af50dbfdfc435ab9604e82562ea3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmps==0.1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: opt_einsum>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from torchmps==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.9/dist-packages (from opt_einsum>=3.3.0->torchmps==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmps==0.1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmps==0.1.0) (16.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmps==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmps==0.1.0) (1.3.0)\n",
            "fatal: destination path 'honor_project' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install \"git+https://github.com/ScierKnave/TorchMPS.git\"\n",
        "!git clone \"https://github.com/ScierKnave/honor_project.git\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.modules.pooling import MaxPool2d\n",
        "%pip install torchmetrics\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from torchmps import MPS\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4cYNyo4nE3j"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DtL7C860NJlr"
      },
      "outputs": [],
      "source": [
        "# FC to MPS\n",
        "\n",
        "# Hardware hyperparameters\n",
        "chosen_device = torch.device('cuda' \n",
        "if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data hyperparameters\n",
        "nb_train_HP = 2000\n",
        "nb_test_HP = 500\n",
        "batch_sz_HP = 150\n",
        "batch_sz_HP = min(batch_sz_HP, nb_train_HP)\n",
        "nb_classes_HP = 10\n",
        "\n",
        "# Teacher hyperparameters\n",
        "nepochs_teacher_HP = 25\n",
        "teacher_loss_HP = nn.CrossEntropyLoss()\n",
        "teacher_lr_HP = 1e-2\n",
        "teacher_reg_HP = 0.01\n",
        "teacher_hidden_size_HP = 70\n",
        "# Student hyperparameters\n",
        "# MPS parameters\n",
        "bond_dim_HP = 40\n",
        "adaptive_mode_HP = False\n",
        "periodic_bc_HP = False\n",
        "\n",
        "# USING CUSTOM FEATURE MAP WITH FORK?: YES\n",
        "\n",
        "# Training parameters\n",
        "nepochs_student_HP = 25 \n",
        "student_lr_HP = 1e-4\n",
        "student_reg_HP = 0.01\n",
        "student_loss_HP = nn.KLDivLoss(reduction = \"batchmean\", log_target = True)\n",
        "\n",
        "# Gaussian parameters\n",
        "gauss_epochs_HP = 0 # number of epochs with added gaussian noise\n",
        "gn_var_HP = 0.3 #added gaussian noise variance\n",
        "gn_mean_HP = 0 #added gaussian noise mean\n",
        "nepochs_student_HP = 25 + gauss_epochs_HP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvJCqr8QnE3l"
      },
      "source": [
        "# Premilinaries: Importing the data and utils subroutines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "34pmpVgSOBhB"
      },
      "outputs": [],
      "source": [
        "# Import the *common* mnist train set and create a batch iterator for it.\n",
        "train_set = torch.load('/content/honor_project/src/datasets/train_mnist.pt')\n",
        "train_iterator = torch.utils.data.DataLoader(\n",
        "    dataset = train_set, \n",
        "    sampler = torch.utils.data.SubsetRandomSampler(range(nb_train_HP)),\n",
        "    batch_size=batch_sz_HP\n",
        "    )\n",
        "\n",
        "# Import the mnist *common* test set and create a batch iterator for it.\n",
        "test_set = torch.load('/content/honor_project/src/datasets/test_mnist.pt')\n",
        "test_iterator = torch.utils.data.DataLoader(\n",
        "    dataset = test_set, \n",
        "    sampler = torch.utils.data.SubsetRandomSampler(range(nb_test_HP)),\n",
        "    batch_size = batch_sz_HP\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NlVDkr_-nE3l"
      },
      "outputs": [],
      "source": [
        "# Returns the validation set classification accuracy\n",
        "# of the given input model (this is a higher order function)\n",
        "def get_acc(model, iterator, dataset_size):\n",
        "    # Get the validation set classification accuracy\n",
        "    total_good_classifications = 0\n",
        "    acc_metric = MulticlassAccuracy(num_classes=nb_classes_HP).to(chosen_device)\n",
        "    for (x_mb, y_mb) in iterator:\n",
        "        x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "        y_mb = y_mb.to(chosen_device)\n",
        "        # Add the number of datapoints we classified right to the total\n",
        "        batch_size = x_mb.size()[0]\n",
        "        y_hat = model(x_mb)\n",
        "        batch_good_classifications = batch_size * acc_metric(y_hat, y_mb)\n",
        "        total_good_classifications += batch_good_classifications\n",
        "    return total_good_classifications / dataset_size # divide by total size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the fcnn class\n",
        "class FCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lin1 = nn.Linear(784, teacher_hidden_size_HP)\n",
        "        self.lin2 = nn.Linear(teacher_hidden_size_HP, teacher_hidden_size_HP)\n",
        "        self.lin3 = nn.Linear(teacher_hidden_size_HP, teacher_hidden_size_HP)\n",
        "        self.lin4 = nn.Linear(teacher_hidden_size_HP, teacher_hidden_size_HP)\n",
        "        self.lin5 = nn.Linear(teacher_hidden_size_HP, teacher_hidden_size_HP)\n",
        "        self.lin6 = nn.Linear(teacher_hidden_size_HP, 10)\n",
        "\n",
        "    def middleforward(self, x):\n",
        "        y = self.lin1(x)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin2(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin3(y)\n",
        "        y = self.relu(y)\n",
        "        return y\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.lin1(x)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin2(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin3(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin4(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin5(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin6(y)\n",
        "        y = self.relu(y)\n",
        "        return y\n",
        "\n",
        "teacher = torch.load('/content/honor_project/src/fc_0.541.pt')\n"
      ],
      "metadata": {
        "id": "F-h7WlXnnmJV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acY6Jwc7nE3o"
      },
      "source": [
        "# Training the student model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZKdeyavpUl2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73fc4dbb-f8c9-4806-e5a0-70c60e5208d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training and testing of model  1  in progress...\n",
            "Train loss:  [0.1    0.1    0.1939 0.396  0.3932 0.4715 0.4974 0.536  0.5255 0.5394\n",
            " 0.5588 0.5526 0.5745 0.5709 0.5784 0.581  0.579  0.5823 0.5911 0.5824\n",
            " 0.5844 0.6003 0.5823 0.5865 0.5913]\n",
            "Test loss:  [0.1    0.1    0.1846 0.3839 0.3929 0.4484 0.482  0.521  0.5187 0.5368\n",
            " 0.5423 0.5302 0.5505 0.5549 0.5519 0.5394 0.5469 0.5489 0.554  0.5573\n",
            " 0.5638 0.5755 0.5533 0.5574 0.5662]\n",
            "\n",
            " Training and testing of model  2  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1778 0.1    0.1    0.1    0.1    0.1\n",
            " 0.2175 0.2862 0.4188 0.468  0.5136 0.5072 0.5445 0.5611 0.5657 0.5732\n",
            " 0.5646 0.5791 0.5784 0.5865 0.5809]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1865 0.1    0.1    0.1    0.1    0.1\n",
            " 0.2182 0.2799 0.4002 0.4698 0.4938 0.513  0.5312 0.5362 0.5472 0.5511\n",
            " 0.5514 0.5467 0.5527 0.5497 0.5461]\n",
            "\n",
            " Training and testing of model  3  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.2336 0.3104 0.3583 0.4675 0.5234 0.5264\n",
            " 0.5387 0.5577 0.5578 0.5712 0.5777 0.57   0.5793 0.5797 0.5815 0.5912\n",
            " 0.5838 0.5832 0.5814 0.5888 0.5918]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.2561 0.3194 0.3664 0.4589 0.5293 0.5402\n",
            " 0.5387 0.5431 0.5423 0.5543 0.5606 0.541  0.5523 0.5441 0.5644 0.5662\n",
            " 0.5458 0.5579 0.5621 0.5551 0.5575]\n",
            "\n",
            " Training and testing of model  4  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1617 0.3829 0.4654 0.4881 0.4885 0.5278 0.5254\n",
            " 0.5625 0.5628 0.57   0.5721 0.5658 0.5792 0.5783 0.5764 0.5825 0.5851\n",
            " 0.5875 0.585  0.5881 0.5894 0.5836]\n",
            "Test loss:  [0.1    0.1    0.1    0.1574 0.3811 0.482  0.483  0.4836 0.5256 0.5035\n",
            " 0.5447 0.5566 0.5391 0.5568 0.5397 0.5576 0.5537 0.5444 0.5446 0.552\n",
            " 0.5554 0.5521 0.5515 0.5481 0.5529]\n",
            "\n",
            " Training and testing of model  5  in progress...\n",
            "Train loss:  [0.1    0.1    0.1018 0.2304 0.3317 0.4192 0.4849 0.5344 0.5342 0.5489\n",
            " 0.5635 0.5717 0.5678 0.5814 0.581  0.58   0.5577 0.5832 0.5859 0.5857\n",
            " 0.5882 0.5918 0.5882 0.5904 0.5897]\n",
            "Test loss:  [0.1    0.1    0.1    0.2412 0.3402 0.4058 0.474  0.5208 0.5271 0.531\n",
            " 0.5464 0.5512 0.5545 0.566  0.5634 0.5562 0.5293 0.5611 0.5642 0.5502\n",
            " 0.5617 0.5637 0.5605 0.5571 0.5603]\n",
            "\n",
            " Training and testing of model  6  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1    0.1    0.1814 0.3356 0.4579 0.4645\n",
            " 0.5008 0.5317 0.54   0.5369 0.559  0.5695 0.5628 0.5725 0.5783 0.58\n",
            " 0.5843 0.5836 0.5843 0.5838 0.581 ]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1    0.1    0.2027 0.3328 0.4467 0.4662\n",
            " 0.4889 0.5144 0.5319 0.526  0.5416 0.5491 0.5319 0.5453 0.5459 0.5495\n",
            " 0.5444 0.5379 0.5361 0.5495 0.541 ]\n",
            "\n",
            " Training and testing of model  7  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1354 0.1606 0.3663 0.3803 0.4474 0.5086 0.5128\n",
            " 0.5581 0.557  0.5644 0.5797 0.578  0.5764 0.5709 0.5843 0.5835 0.5865\n",
            " 0.5856 0.5885 0.5871 0.5958 0.5876]\n",
            "Test loss:  [0.1    0.1    0.1    0.1334 0.1495 0.365  0.3832 0.4477 0.505  0.492\n",
            " 0.5423 0.5419 0.5407 0.5558 0.548  0.5472 0.5501 0.5581 0.5587 0.5671\n",
            " 0.5563 0.5572 0.5586 0.5753 0.5622]\n",
            "\n",
            " Training and testing of model  8  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.164  0.2887 0.3927 0.4321 0.5082 0.5265 0.4973\n",
            " 0.5179 0.5558 0.5663 0.5742 0.5826 0.5833 0.5788 0.584  0.5812 0.5861\n",
            " 0.5825 0.5828 0.5859 0.5839 0.5913]\n",
            "Test loss:  [0.1    0.1    0.1    0.1741 0.2843 0.3709 0.435  0.5152 0.5239 0.4898\n",
            " 0.5252 0.5304 0.5376 0.5564 0.5519 0.537  0.5451 0.5498 0.5564 0.5553\n",
            " 0.5533 0.5491 0.5503 0.5484 0.5526]\n",
            "\n",
            " Training and testing of model  9  in progress...\n",
            "Train loss:  [0.1    0.1    0.1685 0.2774 0.4322 0.4656 0.496  0.5161 0.5208 0.5465\n",
            " 0.5598 0.5548 0.5602 0.5731 0.5743 0.5766 0.5775 0.5826 0.5779 0.5867\n",
            " 0.5843 0.5909 0.5887 0.5881 0.5875]\n",
            "Test loss:  [0.1    0.1    0.169  0.2827 0.441  0.4658 0.4942 0.5124 0.4997 0.541\n",
            " 0.5336 0.5415 0.5505 0.556  0.5518 0.5618 0.5528 0.5504 0.55   0.5614\n",
            " 0.5677 0.5694 0.5584 0.5628 0.5598]\n",
            "\n",
            " Training and testing of model  10  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1    0.1769 0.3398 0.4454 0.4535 0.4933\n",
            " 0.5274 0.523  0.5558 0.5582 0.5581 0.5679 0.573  0.5763 0.5862 0.5783\n",
            " 0.5842 0.5829 0.5831 0.5831 0.5768]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1    0.19   0.3481 0.436  0.4499 0.483\n",
            " 0.5226 0.5163 0.5526 0.5486 0.5427 0.5548 0.5443 0.5517 0.5565 0.5385\n",
            " 0.5502 0.548  0.5577 0.5505 0.555 ]\n",
            "\n",
            " Training and testing of model  11  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.2002 0.2892 0.3892 0.414  0.4734 0.4934 0.5154\n",
            " 0.4917 0.5415 0.5458 0.569  0.5704 0.571  0.5668 0.5778 0.5692 0.5833\n",
            " 0.582  0.5784 0.5898 0.5869 0.5923]\n",
            "Test loss:  [0.1    0.1    0.1    0.1934 0.2962 0.3913 0.3863 0.4585 0.4783 0.5074\n",
            " 0.4779 0.5273 0.524  0.5405 0.5356 0.5408 0.5415 0.5578 0.5378 0.5449\n",
            " 0.558  0.5442 0.5616 0.5435 0.5649]\n",
            "\n",
            " Training and testing of model  12  in progress...\n",
            "Train loss:  [0.1    0.1    0.1566 0.1736 0.2552 0.3947 0.4588 0.4879 0.5112 0.5377\n",
            " 0.5331 0.5587 0.5676 0.5553 0.5634 0.564  0.5754 0.574  0.5713 0.5816\n",
            " 0.5873 0.5942 0.5806 0.5926 0.5926]\n",
            "Test loss:  [0.1    0.1    0.1594 0.1817 0.2456 0.3754 0.4344 0.4855 0.4989 0.5301\n",
            " 0.5206 0.5412 0.5437 0.5253 0.5484 0.5362 0.5421 0.5348 0.5398 0.5483\n",
            " 0.5503 0.5743 0.5494 0.5607 0.5612]\n",
            "\n",
            " Training and testing of model  13  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1    0.1    0.1004 0.2074 0.3093 0.3694\n",
            " 0.4308 0.4945 0.5065 0.5308 0.5325 0.566  0.5654 0.535  0.5688 0.5746\n",
            " 0.5786 0.5844 0.579  0.585  0.5867]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1    0.1    0.1019 0.2215 0.3257 0.4043\n",
            " 0.4343 0.5149 0.4996 0.5188 0.5279 0.5482 0.5501 0.5205 0.5627 0.5561\n",
            " 0.5531 0.5444 0.555  0.556  0.5571]\n",
            "\n",
            " Training and testing of model  14  in progress...\n",
            "Train loss:  [0.1    0.2631 0.2988 0.3528 0.4289 0.4867 0.4784 0.5352 0.5693 0.5525\n",
            " 0.5714 0.5746 0.5767 0.5838 0.5861 0.5906 0.5808 0.5815 0.5768 0.5869\n",
            " 0.5851 0.5889 0.5916 0.5901 0.5912]\n",
            "Test loss:  [0.1    0.2558 0.2962 0.3675 0.4387 0.4794 0.4667 0.5323 0.559  0.5422\n",
            " 0.5498 0.5526 0.5512 0.5531 0.5597 0.5562 0.5611 0.5576 0.5551 0.5595\n",
            " 0.5697 0.5579 0.5653 0.5616 0.5666]\n",
            "\n",
            " Training and testing of model  15  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1243 0.1952 0.2628 0.4398 0.4772 0.4869\n",
            " 0.5034 0.535  0.5459 0.5553 0.563  0.568  0.577  0.5742 0.5741 0.5777\n",
            " 0.5797 0.5815 0.5833 0.5859 0.5867]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1312 0.1921 0.2602 0.4306 0.49   0.4908\n",
            " 0.4936 0.536  0.538  0.5311 0.5432 0.5428 0.5492 0.5506 0.5426 0.5414\n",
            " 0.5464 0.5488 0.5577 0.551  0.5399]\n",
            "\n",
            " Training and testing of model  16  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1591 0.2537 0.4564 0.4927 0.498  0.5344 0.5581\n",
            " 0.5563 0.5558 0.5704 0.5723 0.584  0.5837 0.5828 0.585  0.5798 0.5832\n",
            " 0.5869 0.5858 0.589  0.5896 0.592 ]\n",
            "Test loss:  [0.1    0.1    0.1    0.1652 0.2295 0.4835 0.4914 0.4914 0.5154 0.5412\n",
            " 0.542  0.5411 0.545  0.5441 0.5537 0.5605 0.5569 0.553  0.5377 0.5574\n",
            " 0.5562 0.5635 0.5432 0.5593 0.5647]\n",
            "\n",
            " Training and testing of model  17  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.2776 0.3581 0.4346 0.4901 0.524  0.5515\n",
            " 0.5323 0.5243 0.5681 0.5707 0.5766 0.5796 0.5776 0.5793 0.5816 0.5851\n",
            " 0.5855 0.5829 0.5842 0.5813 0.5922]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.2687 0.355  0.4281 0.4789 0.5205 0.5403\n",
            " 0.528  0.5041 0.5447 0.5394 0.5444 0.5524 0.5465 0.5479 0.5542 0.5636\n",
            " 0.5481 0.5428 0.5391 0.5508 0.5533]\n",
            "\n",
            " Training and testing of model  18  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.17   0.242  0.3646 0.4582 0.467  0.5086 0.537\n",
            " 0.5381 0.5666 0.5539 0.5664 0.5716 0.564  0.5868 0.5797 0.5838 0.5842\n",
            " 0.5853 0.5866 0.5907 0.5891 0.5904]\n",
            "Test loss:  [0.1    0.1    0.1    0.1699 0.2575 0.3664 0.4442 0.4685 0.4974 0.5297\n",
            " 0.5256 0.5493 0.5305 0.5521 0.5463 0.5471 0.551  0.5603 0.5611 0.5528\n",
            " 0.5628 0.5524 0.5588 0.5638 0.5508]\n",
            "\n",
            " Training and testing of model  19  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1093 0.2292 0.4    0.4646 0.4901 0.5253\n",
            " 0.5252 0.5568 0.562  0.5568 0.5681 0.5653 0.5783 0.5732 0.5808 0.5786\n",
            " 0.5719 0.5821 0.5912 0.5842 0.5845]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.112  0.2454 0.3813 0.4534 0.4787 0.5136\n",
            " 0.5016 0.5415 0.5364 0.5404 0.5534 0.5432 0.557  0.5529 0.5495 0.5527\n",
            " 0.5441 0.5666 0.5557 0.5563 0.5586]\n",
            "\n",
            " Training and testing of model  20  in progress...\n",
            "Train loss:  [0.1    0.1    0.1733 0.2206 0.3976 0.4451 0.4831 0.524  0.5356 0.5543\n",
            " 0.5571 0.5616 0.5589 0.5776 0.5627 0.5759 0.5862 0.5814 0.5816 0.5786\n",
            " 0.5815 0.5835 0.5784 0.5814 0.59  ]\n",
            "Test loss:  [0.1    0.1    0.1772 0.2198 0.3755 0.446  0.4642 0.518  0.5257 0.5329\n",
            " 0.5432 0.5403 0.5402 0.5583 0.5347 0.5527 0.5495 0.5452 0.5536 0.5531\n",
            " 0.5499 0.5534 0.548  0.5536 0.547 ]\n"
          ]
        }
      ],
      "source": [
        "def train_a_student():\n",
        "  '''\n",
        "  Trains a student model.\n",
        "  '''\n",
        "  # Initialize the MPS module\n",
        "  student = MPS(\n",
        "      input_dim = 28 ** 2,\n",
        "      output_dim = 10,\n",
        "      bond_dim = bond_dim_HP\n",
        "  ).to(chosen_device)\n",
        "  #student.register_feature_map(feature_map_HP)\n",
        "\n",
        "  # Instantiate the optimizer and softmax\n",
        "  student_optimizer = torch.optim.Adam(\n",
        "      student.parameters(), lr = student_lr_HP, weight_decay = student_reg_HP\n",
        "  )\n",
        "\n",
        "  # Used on the inputs before the loss function\n",
        "  LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  # Create an array to store the val loss\n",
        "  # of the student at each epoch\n",
        "  stud_test_loss = np.array([])\n",
        "  stud_train_loss = np.array([])\n",
        "\n",
        "  # Training loop \n",
        "  for epoch in range(nepochs_student_HP):\n",
        "      for (x_mb, y_mb) in train_iterator:\n",
        "          # Flatten the MNIST images, which come in matrix form\n",
        "          x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "          y_mb = y_mb.to(chosen_device)\n",
        "\n",
        "          # Add Gaussian noise for the gaussian epochs\n",
        "          if (epoch >= nepochs_student_HP - gauss_epochs_HP):\n",
        "            x_mb = x_mb + torch.randn(size=x_mb.size()).to(chosen_device)\n",
        "\n",
        "          student_output = LogSoftmax( student(x_mb) )\n",
        "          teacher_output = LogSoftmax( teacher(x_mb) )\n",
        "\n",
        "          # Backpropagation\n",
        "          loss = student_loss_HP(student_output, teacher_output)\n",
        "          loss.backward()\n",
        "          student_optimizer.step()\n",
        "          student_optimizer.zero_grad()\n",
        "\n",
        "      # Get accuracy over all test and training data for current epoch\n",
        "      train_current_accuracy = round( get_acc(student, train_iterator, nb_train_HP).item(), 4)\n",
        "      test_current_accuracy = round( get_acc(student, test_iterator, nb_test_HP).item(), 4)\n",
        "      stud_train_loss = np.append(stud_train_loss, train_current_accuracy)\n",
        "      stud_test_loss = np.append(stud_test_loss, test_current_accuracy)\n",
        "  return(stud_train_loss, stud_test_loss)\n",
        "\n",
        "# Repeat the training process in order to get the variance\n",
        "global_stud_test_loss = np.array([])\n",
        "global_stud_train_loss = np.array([])\n",
        "for i in range(20):\n",
        "  print(\"\\n Training and testing of model \", i+1, \" in progress...\")\n",
        "  (stud_train_loss, stud_test_loss) = train_a_student()\n",
        "  print(\"Train loss: \", stud_train_loss)\n",
        "  print(\"Test loss: \", stud_test_loss)\n",
        "  if (i == 0):\n",
        "    global_stud_train_loss = stud_train_loss\n",
        "    global_stud_test_loss = stud_test_loss\n",
        "  else:\n",
        "    global_stud_train_loss = np.vstack((global_stud_train_loss, stud_train_loss))\n",
        "    global_stud_test_loss = np.vstack((global_stud_test_loss, stud_test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the final results and save them for the report.\n",
        "print(\"Final results\")\n",
        "print(\"Train loss: \", global_stud_train_loss)\n",
        "print(\"Test loss: \", global_stud_test_loss)\n",
        "\n",
        "np.save('20x_fc_to_mps_trainloss_' + str(bond_dim_HP) + 'bd', global_stud_train_loss)\n",
        "np.save('20x_fc_to_mps_testloss_' + str(bond_dim_HP) + 'bd', global_stud_test_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "RQr9AIrMzJ1A",
        "outputId": "f61de1da-1065-4dc8-e182-5f145f32acee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results\n",
            "Train loss:  [[0.1    0.1    0.1939 0.396  0.3932 0.4715 0.4974 0.536  0.5255 0.5394\n",
            "  0.5588 0.5526 0.5745 0.5709 0.5784 0.581  0.579  0.5823 0.5911 0.5824\n",
            "  0.5844 0.6003 0.5823 0.5865 0.5913]\n",
            " [0.1    0.1    0.1    0.1    0.1778 0.1    0.1    0.1    0.1    0.1\n",
            "  0.2175 0.2862 0.4188 0.468  0.5136 0.5072 0.5445 0.5611 0.5657 0.5732\n",
            "  0.5646 0.5791 0.5784 0.5865 0.5809]\n",
            " [0.1    0.1    0.1    0.1    0.2336 0.3104 0.3583 0.4675 0.5234 0.5264\n",
            "  0.5387 0.5577 0.5578 0.5712 0.5777 0.57   0.5793 0.5797 0.5815 0.5912\n",
            "  0.5838 0.5832 0.5814 0.5888 0.5918]\n",
            " [0.1    0.1    0.1    0.1617 0.3829 0.4654 0.4881 0.4885 0.5278 0.5254\n",
            "  0.5625 0.5628 0.57   0.5721 0.5658 0.5792 0.5783 0.5764 0.5825 0.5851\n",
            "  0.5875 0.585  0.5881 0.5894 0.5836]\n",
            " [0.1    0.1    0.1018 0.2304 0.3317 0.4192 0.4849 0.5344 0.5342 0.5489\n",
            "  0.5635 0.5717 0.5678 0.5814 0.581  0.58   0.5577 0.5832 0.5859 0.5857\n",
            "  0.5882 0.5918 0.5882 0.5904 0.5897]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1    0.1814 0.3356 0.4579 0.4645\n",
            "  0.5008 0.5317 0.54   0.5369 0.559  0.5695 0.5628 0.5725 0.5783 0.58\n",
            "  0.5843 0.5836 0.5843 0.5838 0.581 ]\n",
            " [0.1    0.1    0.1    0.1354 0.1606 0.3663 0.3803 0.4474 0.5086 0.5128\n",
            "  0.5581 0.557  0.5644 0.5797 0.578  0.5764 0.5709 0.5843 0.5835 0.5865\n",
            "  0.5856 0.5885 0.5871 0.5958 0.5876]\n",
            " [0.1    0.1    0.1    0.164  0.2887 0.3927 0.4321 0.5082 0.5265 0.4973\n",
            "  0.5179 0.5558 0.5663 0.5742 0.5826 0.5833 0.5788 0.584  0.5812 0.5861\n",
            "  0.5825 0.5828 0.5859 0.5839 0.5913]\n",
            " [0.1    0.1    0.1685 0.2774 0.4322 0.4656 0.496  0.5161 0.5208 0.5465\n",
            "  0.5598 0.5548 0.5602 0.5731 0.5743 0.5766 0.5775 0.5826 0.5779 0.5867\n",
            "  0.5843 0.5909 0.5887 0.5881 0.5875]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1769 0.3398 0.4454 0.4535 0.4933\n",
            "  0.5274 0.523  0.5558 0.5582 0.5581 0.5679 0.573  0.5763 0.5862 0.5783\n",
            "  0.5842 0.5829 0.5831 0.5831 0.5768]\n",
            " [0.1    0.1    0.1    0.2002 0.2892 0.3892 0.414  0.4734 0.4934 0.5154\n",
            "  0.4917 0.5415 0.5458 0.569  0.5704 0.571  0.5668 0.5778 0.5692 0.5833\n",
            "  0.582  0.5784 0.5898 0.5869 0.5923]\n",
            " [0.1    0.1    0.1566 0.1736 0.2552 0.3947 0.4588 0.4879 0.5112 0.5377\n",
            "  0.5331 0.5587 0.5676 0.5553 0.5634 0.564  0.5754 0.574  0.5713 0.5816\n",
            "  0.5873 0.5942 0.5806 0.5926 0.5926]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1    0.1004 0.2074 0.3093 0.3694\n",
            "  0.4308 0.4945 0.5065 0.5308 0.5325 0.566  0.5654 0.535  0.5688 0.5746\n",
            "  0.5786 0.5844 0.579  0.585  0.5867]\n",
            " [0.1    0.2631 0.2988 0.3528 0.4289 0.4867 0.4784 0.5352 0.5693 0.5525\n",
            "  0.5714 0.5746 0.5767 0.5838 0.5861 0.5906 0.5808 0.5815 0.5768 0.5869\n",
            "  0.5851 0.5889 0.5916 0.5901 0.5912]\n",
            " [0.1    0.1    0.1    0.1    0.1243 0.1952 0.2628 0.4398 0.4772 0.4869\n",
            "  0.5034 0.535  0.5459 0.5553 0.563  0.568  0.577  0.5742 0.5741 0.5777\n",
            "  0.5797 0.5815 0.5833 0.5859 0.5867]\n",
            " [0.1    0.1    0.1    0.1591 0.2537 0.4564 0.4927 0.498  0.5344 0.5581\n",
            "  0.5563 0.5558 0.5704 0.5723 0.584  0.5837 0.5828 0.585  0.5798 0.5832\n",
            "  0.5869 0.5858 0.589  0.5896 0.592 ]\n",
            " [0.1    0.1    0.1    0.1    0.2776 0.3581 0.4346 0.4901 0.524  0.5515\n",
            "  0.5323 0.5243 0.5681 0.5707 0.5766 0.5796 0.5776 0.5793 0.5816 0.5851\n",
            "  0.5855 0.5829 0.5842 0.5813 0.5922]\n",
            " [0.1    0.1    0.1    0.17   0.242  0.3646 0.4582 0.467  0.5086 0.537\n",
            "  0.5381 0.5666 0.5539 0.5664 0.5716 0.564  0.5868 0.5797 0.5838 0.5842\n",
            "  0.5853 0.5866 0.5907 0.5891 0.5904]\n",
            " [0.1    0.1    0.1    0.1    0.1093 0.2292 0.4    0.4646 0.4901 0.5253\n",
            "  0.5252 0.5568 0.562  0.5568 0.5681 0.5653 0.5783 0.5732 0.5808 0.5786\n",
            "  0.5719 0.5821 0.5912 0.5842 0.5845]\n",
            " [0.1    0.1    0.1733 0.2206 0.3976 0.4451 0.4831 0.524  0.5356 0.5543\n",
            "  0.5571 0.5616 0.5589 0.5776 0.5627 0.5759 0.5862 0.5814 0.5816 0.5786\n",
            "  0.5815 0.5835 0.5784 0.5814 0.59  ]]\n",
            "Test loss:  [[0.1    0.1    0.1846 0.3839 0.3929 0.4484 0.482  0.521  0.5187 0.5368\n",
            "  0.5423 0.5302 0.5505 0.5549 0.5519 0.5394 0.5469 0.5489 0.554  0.5573\n",
            "  0.5638 0.5755 0.5533 0.5574 0.5662]\n",
            " [0.1    0.1    0.1    0.1    0.1865 0.1    0.1    0.1    0.1    0.1\n",
            "  0.2182 0.2799 0.4002 0.4698 0.4938 0.513  0.5312 0.5362 0.5472 0.5511\n",
            "  0.5514 0.5467 0.5527 0.5497 0.5461]\n",
            " [0.1    0.1    0.1    0.1    0.2561 0.3194 0.3664 0.4589 0.5293 0.5402\n",
            "  0.5387 0.5431 0.5423 0.5543 0.5606 0.541  0.5523 0.5441 0.5644 0.5662\n",
            "  0.5458 0.5579 0.5621 0.5551 0.5575]\n",
            " [0.1    0.1    0.1    0.1574 0.3811 0.482  0.483  0.4836 0.5256 0.5035\n",
            "  0.5447 0.5566 0.5391 0.5568 0.5397 0.5576 0.5537 0.5444 0.5446 0.552\n",
            "  0.5554 0.5521 0.5515 0.5481 0.5529]\n",
            " [0.1    0.1    0.1    0.2412 0.3402 0.4058 0.474  0.5208 0.5271 0.531\n",
            "  0.5464 0.5512 0.5545 0.566  0.5634 0.5562 0.5293 0.5611 0.5642 0.5502\n",
            "  0.5617 0.5637 0.5605 0.5571 0.5603]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1    0.2027 0.3328 0.4467 0.4662\n",
            "  0.4889 0.5144 0.5319 0.526  0.5416 0.5491 0.5319 0.5453 0.5459 0.5495\n",
            "  0.5444 0.5379 0.5361 0.5495 0.541 ]\n",
            " [0.1    0.1    0.1    0.1334 0.1495 0.365  0.3832 0.4477 0.505  0.492\n",
            "  0.5423 0.5419 0.5407 0.5558 0.548  0.5472 0.5501 0.5581 0.5587 0.5671\n",
            "  0.5563 0.5572 0.5586 0.5753 0.5622]\n",
            " [0.1    0.1    0.1    0.1741 0.2843 0.3709 0.435  0.5152 0.5239 0.4898\n",
            "  0.5252 0.5304 0.5376 0.5564 0.5519 0.537  0.5451 0.5498 0.5564 0.5553\n",
            "  0.5533 0.5491 0.5503 0.5484 0.5526]\n",
            " [0.1    0.1    0.169  0.2827 0.441  0.4658 0.4942 0.5124 0.4997 0.541\n",
            "  0.5336 0.5415 0.5505 0.556  0.5518 0.5618 0.5528 0.5504 0.55   0.5614\n",
            "  0.5677 0.5694 0.5584 0.5628 0.5598]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.19   0.3481 0.436  0.4499 0.483\n",
            "  0.5226 0.5163 0.5526 0.5486 0.5427 0.5548 0.5443 0.5517 0.5565 0.5385\n",
            "  0.5502 0.548  0.5577 0.5505 0.555 ]\n",
            " [0.1    0.1    0.1    0.1934 0.2962 0.3913 0.3863 0.4585 0.4783 0.5074\n",
            "  0.4779 0.5273 0.524  0.5405 0.5356 0.5408 0.5415 0.5578 0.5378 0.5449\n",
            "  0.558  0.5442 0.5616 0.5435 0.5649]\n",
            " [0.1    0.1    0.1594 0.1817 0.2456 0.3754 0.4344 0.4855 0.4989 0.5301\n",
            "  0.5206 0.5412 0.5437 0.5253 0.5484 0.5362 0.5421 0.5348 0.5398 0.5483\n",
            "  0.5503 0.5743 0.5494 0.5607 0.5612]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1    0.1019 0.2215 0.3257 0.4043\n",
            "  0.4343 0.5149 0.4996 0.5188 0.5279 0.5482 0.5501 0.5205 0.5627 0.5561\n",
            "  0.5531 0.5444 0.555  0.556  0.5571]\n",
            " [0.1    0.2558 0.2962 0.3675 0.4387 0.4794 0.4667 0.5323 0.559  0.5422\n",
            "  0.5498 0.5526 0.5512 0.5531 0.5597 0.5562 0.5611 0.5576 0.5551 0.5595\n",
            "  0.5697 0.5579 0.5653 0.5616 0.5666]\n",
            " [0.1    0.1    0.1    0.1    0.1312 0.1921 0.2602 0.4306 0.49   0.4908\n",
            "  0.4936 0.536  0.538  0.5311 0.5432 0.5428 0.5492 0.5506 0.5426 0.5414\n",
            "  0.5464 0.5488 0.5577 0.551  0.5399]\n",
            " [0.1    0.1    0.1    0.1652 0.2295 0.4835 0.4914 0.4914 0.5154 0.5412\n",
            "  0.542  0.5411 0.545  0.5441 0.5537 0.5605 0.5569 0.553  0.5377 0.5574\n",
            "  0.5562 0.5635 0.5432 0.5593 0.5647]\n",
            " [0.1    0.1    0.1    0.1    0.2687 0.355  0.4281 0.4789 0.5205 0.5403\n",
            "  0.528  0.5041 0.5447 0.5394 0.5444 0.5524 0.5465 0.5479 0.5542 0.5636\n",
            "  0.5481 0.5428 0.5391 0.5508 0.5533]\n",
            " [0.1    0.1    0.1    0.1699 0.2575 0.3664 0.4442 0.4685 0.4974 0.5297\n",
            "  0.5256 0.5493 0.5305 0.5521 0.5463 0.5471 0.551  0.5603 0.5611 0.5528\n",
            "  0.5628 0.5524 0.5588 0.5638 0.5508]\n",
            " [0.1    0.1    0.1    0.1    0.112  0.2454 0.3813 0.4534 0.4787 0.5136\n",
            "  0.5016 0.5415 0.5364 0.5404 0.5534 0.5432 0.557  0.5529 0.5495 0.5527\n",
            "  0.5441 0.5666 0.5557 0.5563 0.5586]\n",
            " [0.1    0.1    0.1772 0.2198 0.3755 0.446  0.4642 0.518  0.5257 0.5329\n",
            "  0.5432 0.5403 0.5402 0.5583 0.5347 0.5527 0.5495 0.5452 0.5536 0.5531\n",
            "  0.5499 0.5534 0.548  0.5536 0.547 ]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "67bfac4f4aefe1c16f1836a62d55b6e6baa7aba1ac5ce70e93ee8e90eb4f073a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}