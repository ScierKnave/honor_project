{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_KGIDzqNImk"
      },
      "source": [
        "# Distillation \n",
        "\n",
        "Ce a pour but d'entrainer un MPS multicouche à l'aide d'un réseau complètement connecté"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJYw8wfzOBhA",
        "outputId": "856764be-53b6-482b-9b2c-4f63d5b86c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/jemisjoky/TorchMPS.git\n",
            "  Cloning https://github.com/jemisjoky/TorchMPS.git to /tmp/pip-req-build-g19ez2yt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/jemisjoky/TorchMPS.git /tmp/pip-req-build-g19ez2yt\n",
            "  Resolved https://github.com/jemisjoky/TorchMPS.git to commit 6c0bc1a8e2c15acba8570ca9ffe2b4a0c7135165\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmps==0.1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: opt_einsum>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from torchmps==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.9/dist-packages (from opt_einsum>=3.3.0->torchmps==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install \"git+https://github.com/jemisjoky/TorchMPS.git\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "%pip install torchmetrics\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from torchmps import MPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LwY-QW4nNImq"
      },
      "outputs": [],
      "source": [
        "# Hardware hyperparameters\n",
        "chosen_device = torch.device('cuda' \n",
        "if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data hyperparameters\n",
        "nb_train = 2\n",
        "nb_test = 50\n",
        "chosen_bs = 100\n",
        "chosen_bs = min(nb_train, chosen_bs)\n",
        "input_size = 28*28\n",
        "nb_classes = 10\n",
        "\n",
        "# Teacher hyperparameters\n",
        "chosen_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n_epochs_fcnn = 20\n",
        "hidden_size = 70\n",
        "teacher_loss = nn.CrossEntropyLoss()\n",
        "#Optimizer parameters\n",
        "chosen_lr = 0.001\n",
        "chosen_momentum = 0.9\n",
        "\n",
        "# Student hyperparameters\n",
        "# MPS parameters\n",
        "bond_dim = 20\n",
        "adaptive_mode = False\n",
        "periodic_bc = False\n",
        "feature_map = lambda x : torch.tensor([1, x]).to(chosen_device)\n",
        "feature_dim = 2\n",
        "# Training parameters\n",
        "gauss_epochs = 5 # number of epochs with added gaussian noise\n",
        "gn_var = 0.3 #added gaussian noise variance\n",
        "gn_mean = 0 #added gaussian noise mean\n",
        "n_epochs_lmps = 15\n",
        "mps_learn_rate = 0.0001\n",
        "mps_reg = 0.0\n",
        "mps1_distill_loss = nn.MSELoss()\n",
        "# We choose this loss \n",
        "mps2_distill_loss = nn.KLDivLoss(reduction = \"batchmean\", log_target = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "34pmpVgSOBhB"
      },
      "outputs": [],
      "source": [
        "# Import the mnist dataset\n",
        "train_set = torchvision.datasets.MNIST(root = './datasets', train = True,   \n",
        "    transform = transforms.ToTensor(),  download = True )\n",
        "\n",
        "train_subset = torch.utils.data.SubsetRandomSampler(range(nb_train))\n",
        "\n",
        "train_iterator = torch.utils.data.DataLoader(dataset = train_set, \n",
        "    sampler = train_subset, batch_size=chosen_bs)\n",
        "\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(root = './datasets',\n",
        " train = False, transform = transforms.ToTensor(),  download = True)\n",
        "\n",
        "test_subset = torch.utils.data.SubsetRandomSampler(range(nb_test))\n",
        "\n",
        "test_iterator = torch.utils.data.DataLoader(dataset = test_set, \n",
        "    sampler = test_subset, batch_size = chosen_bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfoH7MNKOBhC",
        "outputId": "55441065-7b3d-4279-b950-52a68daff4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2214975357055664\n",
            "2.209477186203003\n",
            "2.1985538005828857\n",
            "2.187264919281006\n",
            "2.1749372482299805\n",
            "2.161470413208008\n",
            "2.1465814113616943\n",
            "2.129547595977783\n",
            "2.109717607498169\n",
            "2.0863301753997803\n",
            "2.0580015182495117\n",
            "2.0244317054748535\n",
            "1.986724853515625\n",
            "1.9402668476104736\n",
            "1.8847534656524658\n",
            "1.8179469108581543\n",
            "1.7379167079925537\n",
            "1.6422667503356934\n",
            "1.5310845375061035\n",
            "1.4052988290786743\n",
            "The teacher's accuracy score is:\n",
            "tensor(0.0160, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create the fcnn class\n",
        "class FCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lin1 = nn.Linear(784, hidden_size)\n",
        "        self.lin2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.lin3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.lin4 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.lin5 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.lin6 = nn.Linear(hidden_size, 10)\n",
        "\n",
        "    def middleforward(self, x):\n",
        "        y = self.lin1(x)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin2(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin3(y)\n",
        "        y = self.relu(y)\n",
        "        return y\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.lin1(x)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin2(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin3(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin4(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin5(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin6(y)\n",
        "        y = self.relu(y)\n",
        "        return y\n",
        "\n",
        "#Instantiate and put the model on the chosen device\n",
        "fcnn_teacher = FCNN().to(chosen_device)\n",
        "\n",
        "#Instantiate the optimizer\n",
        "optimizer = torch.optim.Adam(fcnn_teacher.parameters())\n",
        "\n",
        "#Training loop\n",
        "for epoch in range(n_epochs_fcnn):\n",
        "    for (x_mb, y_mb) in train_iterator:\n",
        "        # Reshape the train_tuple and put on the chosen device\n",
        "        x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "        y_mb = y_mb.to(chosen_device)\n",
        "        # Foward propagation\n",
        "        y_hat_mb = fcnn_teacher(x_mb)\n",
        "        # We use soft-cross-entropy\n",
        "        # Softmax is precomputed in the loss already in PyTorch\n",
        "        # Ground truth is scalar tensor\n",
        "        loss = teacher_loss(y_hat_mb, y_mb)\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    print(loss.item())\n",
        "\n",
        "# get accuracy\n",
        "# Get the validation set classification accuracy\n",
        "teacher_acc_score = 0\n",
        "teacher_acc_metric = MulticlassAccuracy(num_classes=nb_classes).to(chosen_device)\n",
        "for (x_mb, y_mb) in test_iterator:\n",
        "    x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "    y_mb = y_mb.to(chosen_device)\n",
        "    # add the number of datapoints we classified right\n",
        "    teacher_acc_score += x_mb.size()[0] * teacher_acc_metric( fcnn_teacher(x_mb), y_mb )\n",
        "print(\"The teacher's accuracy score is:\")\n",
        "print(teacher_acc_score / nb_test) #divide by total size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKdeyavpUl2B",
        "outputId": "fbcc21a5-3aeb-470c-9b12-7fbcd38879b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "turn to the second mps\n",
            "0.6938904523849487\n",
            "0.5102578401565552\n",
            "0.37086889147758484\n",
            "0.37950247526168823\n",
            "0.36000439524650574\n",
            "0.36592897772789\n",
            "0.36413097381591797\n",
            "0.35697558522224426\n",
            "0.3576754629611969\n",
            "0.35902753472328186\n",
            "0.3585229516029358\n",
            "0.2780931890010834\n",
            "0.34702497720718384\n",
            "0.3985869586467743\n",
            "0.34828054904937744\n",
            "0.2905879020690918\n",
            "0.29051995277404785\n",
            "0.2904491126537323\n",
            "0.29035505652427673\n",
            "0.2902480959892273\n",
            "0.29010796546936035\n",
            "0.2899174392223358\n",
            "0.28965723514556885\n",
            "0.28929564356803894\n",
            "0.2887907028198242\n",
            "0.28808218240737915\n",
            "0.23274168372154236\n",
            "0.2906225621700287\n",
            "0.3276994228363037\n",
            "0.3096701204776764\n",
            "Student_acc_score:\n",
            "tensor(0.0160, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize the MPS modules\n",
        "student_mps1 = MPS(\n",
        "    input_dim = input_size,\n",
        "    feature_dim = feature_dim,\n",
        "    output_dim = hidden_size,\n",
        "    bond_dim = bond_dim,\n",
        "    adaptive_mode = adaptive_mode,\n",
        "    periodic_bc = periodic_bc,\n",
        ").to(chosen_device)\n",
        "student_mps1.register_feature_map(feature_map)\n",
        "\n",
        "student_mps2 = MPS(\n",
        "    input_dim = hidden_size, #crucial setting\n",
        "    feature_dim = feature_dim,\n",
        "    output_dim = nb_classes,\n",
        "    bond_dim = bond_dim,\n",
        "    adaptive_mode = adaptive_mode,\n",
        "    periodic_bc = periodic_bc,\n",
        ").to(chosen_device)\n",
        "student_mps2.register_feature_map(feature_map)\n",
        "\n",
        "def student(x):\n",
        "    return student_mps2( student_mps1(x) )\n",
        "\n",
        "\n",
        "# Instantiate the optimizer and softmax\n",
        "lmps1_optimizer = torch.optim.Adam(student_mps1.parameters(), \n",
        "    lr = mps_learn_rate, weight_decay = mps_reg)\n",
        "\n",
        "lmps2_optimizer = torch.optim.Adam(student_mps2.parameters(), \n",
        "    lr = mps_learn_rate, weight_decay = mps_reg)\n",
        "\n",
        "print(\"turn to the second mps\")\n",
        "# Training loop for the student_mps1\n",
        "for epoch in range(n_epochs_lmps):\n",
        "    for (x_mb, _) in train_iterator:\n",
        "\n",
        "        # Reshape put on the chosen device\n",
        "        x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "\n",
        "        # Added gaussian noise for the last epochs\n",
        "        if epoch > (n_epochs_lmps - gauss_epochs):\n",
        "          x_mb = x_mb + torch.randn(size=x_mb.size()).to(chosen_device)\n",
        "\n",
        "        # Get output of teacher in middlelayer\n",
        "        # Softmax is precomputed in the loss already\n",
        "        y_mb = fcnn_teacher.middleforward(x_mb).to(chosen_device)\n",
        "\n",
        "        # Foward propagation\n",
        "        y_hat_mb = student_mps1(x_mb)\n",
        "        # We use mean squared error loss\n",
        "        loss = mps1_distill_loss(y_hat_mb, y_mb)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        lmps1_optimizer.step()\n",
        "        lmps1_optimizer.zero_grad()\n",
        "\n",
        "    print(loss.item())\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=0)\n",
        "\n",
        "# Training loop for the student_mps2\n",
        "for epoch in range(n_epochs_lmps):\n",
        "    for (x_mb, _) in train_iterator:\n",
        "        # Reshape put on the chosen device\n",
        "        x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "\n",
        "        # Added gaussian noise\n",
        "        if epoch > (n_epochs_lmps - gauss_epochs):\n",
        "          x_mb = x_mb + torch.randn(size=x_mb.size()).to(chosen_device)\n",
        "\n",
        "        # Get log_softmax of logit outputs of teacher\n",
        "        teacher_output = fcnn_teacher(x_mb)\n",
        "        #teacher_output = torch.logit(fcnn_teacher(x_mb))\n",
        "        teacher_output = nn.functional.log_softmax( teacher_output, dim=1 ) \n",
        "        # Get Log_softmax of outputs of student\n",
        "        student_output = student(x_mb)\n",
        "        #student_output = torch.logit( student(x_mb) )\n",
        "        student_output = nn.functional.log_softmax( student_output, dim=1 ) \n",
        "\n",
        "        # Chosen loss is Kull divergence\n",
        "        loss = mps2_distill_loss(student_output, teacher_output)\n",
        "        \n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        lmps2_optimizer.step()\n",
        "        lmps2_optimizer.zero_grad()\n",
        "\n",
        "    print(loss.item())\n",
        "\n",
        "\n",
        "\n",
        "# Get the validation set classification accuracy\n",
        "student_acc_score = 0\n",
        "student_acc_metric = MulticlassAccuracy(num_classes=nb_classes).to(chosen_device)\n",
        "for (x_mb, y_mb) in test_iterator:\n",
        "    x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "    y_mb = y_mb.to(chosen_device)\n",
        "    # add the number of datapoints we classified right\n",
        "    student_acc_score += x_mb.size()[0] * student_acc_metric( student(x_mb), y_mb )\n",
        "print(\"Student_acc_score:\")\n",
        "print(student_acc_score / nb_test) #divide by total size\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "67bfac4f4aefe1c16f1836a62d55b6e6baa7aba1ac5ce70e93ee8e90eb4f073a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}