{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJYw8wfzOBhA",
        "outputId": "ae67db52-a7e8-448f-9bc9-4c900962b746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ScierKnave/TorchMPS.git\n",
            "  Cloning https://github.com/ScierKnave/TorchMPS.git to /tmp/pip-req-build-03wp9zf7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ScierKnave/TorchMPS.git /tmp/pip-req-build-03wp9zf7\n",
            "  Resolved https://github.com/ScierKnave/TorchMPS.git to commit f716a08e15d0af50dbfdfc435ab9604e82562ea3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmps==0.1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: opt_einsum>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from torchmps==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.9/dist-packages (from opt_einsum>=3.3.0->torchmps==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmps==0.1.0) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmps==0.1.0) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmps==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmps==0.1.0) (1.3.0)\n",
            "Building wheels for collected packages: torchmps\n",
            "  Building wheel for torchmps (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchmps: filename=torchmps-0.1.0-py3-none-any.whl size=59680 sha256=9bdef27a4da62bead27461d2a96bb29c02c24f51af34664a3e74d6a1e56083d6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6iicxqgc/wheels/7c/97/ed/e7cfb0c73fec613f09c365db5a9b684875fdb85e8944240efc\n",
            "Successfully built torchmps\n",
            "Installing collected packages: torchmps\n",
            "Successfully installed torchmps-0.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ],
      "source": [
        "%pip install \"git+https://github.com/ScierKnave/TorchMPS.git\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "%pip install torchmetrics\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from torchmps import MPS\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4cYNyo4nE3j"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DtL7C860NJlr"
      },
      "outputs": [],
      "source": [
        "# FC to MPS\n",
        "\n",
        "# Hardware hyperparameters\n",
        "chosen_device = torch.device('cuda' \n",
        "if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data hyperparameters\n",
        "nb_train_HP = 2000\n",
        "nb_test_HP = 500\n",
        "batch_sz_HP = 150\n",
        "batch_sz_HP = min(batch_sz_HP, nb_train_HP)\n",
        "nb_classes_HP = 10\n",
        "\n",
        "# Teacher hyperparameters\n",
        "nepochs_teacher_HP = 25\n",
        "teacher_loss_HP = nn.CrossEntropyLoss()\n",
        "teacher_lr_HP = 1e-2\n",
        "teacher_reg_HP = 0.01\n",
        "teacher_hidden_size_HP = 70\n",
        "# Student hyperparameters\n",
        "# MPS parameters\n",
        "bond_dim_HP = 40\n",
        "adaptive_mode_HP = False\n",
        "periodic_bc_HP = False\n",
        "\n",
        "# USING CUSTOM FEATURE MAP WITH FORK?: YES\n",
        "\n",
        "# Training parameters\n",
        "nepochs_student_HP = 25 \n",
        "student_lr_HP = 1e-4\n",
        "student_reg_HP = 0.01\n",
        "student_loss_HP = nn.KLDivLoss(reduction = \"batchmean\", log_target = True)\n",
        "\n",
        "# Gaussian parameters\n",
        "gauss_epochs_HP = 0 # number of epochs with added gaussian noise\n",
        "gn_var_HP = 0.3 #added gaussian noise variance\n",
        "gn_mean_HP = 0 #added gaussian noise mean\n",
        "nepochs_student_HP = 25 + gauss_epochs_HP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvJCqr8QnE3l"
      },
      "source": [
        "# Premilinaries: Importing the data and utils subroutines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "34pmpVgSOBhB",
        "outputId": "ebfe29ce-9680-4b7b-f605-bc0337e0ba02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 196332256.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 42728639.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 66214995.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6755506.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the mnist train dataset\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root = './datasets', train = True,   \n",
        "    transform = transforms.ToTensor(),  download = True )\n",
        "\n",
        "# Create a training batch iterator\n",
        "train_subset = torch.utils.data.SubsetRandomSampler(range(nb_train_HP))\n",
        "train_iterator = torch.utils.data.DataLoader(\n",
        "    dataset = train_set, \n",
        "    sampler = train_subset, batch_size=batch_sz_HP\n",
        "    )\n",
        "\n",
        "# Import the mnist test set\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root = './datasets',\n",
        "    train = False, transform = transforms.ToTensor(),  download = True\n",
        "    )\n",
        "# Create a testing batch iterator\n",
        "test_subset = torch.utils.data.SubsetRandomSampler(range(nb_test_HP))\n",
        "test_iterator = torch.utils.data.DataLoader(\n",
        "    dataset = test_set, \n",
        "    sampler = test_subset, batch_size = batch_sz_HP\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NlVDkr_-nE3l"
      },
      "outputs": [],
      "source": [
        "# Returns the validation set classification accuracy\n",
        "# of the given input model (this is a higher order function)\n",
        "def get_acc(model, iterator, dataset_size):\n",
        "    # Get the validation set classification accuracy\n",
        "    total_good_classifications = 0\n",
        "    acc_metric = MulticlassAccuracy(num_classes=nb_classes_HP).to(chosen_device)\n",
        "    for (x_mb, y_mb) in iterator:\n",
        "        x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "        y_mb = y_mb.to(chosen_device)\n",
        "        # Add the number of datapoints we classified right to the total\n",
        "        batch_size = x_mb.size()[0]\n",
        "        y_hat = model(x_mb)\n",
        "        batch_good_classifications = batch_size * acc_metric(y_hat, y_mb)\n",
        "        total_good_classifications += batch_good_classifications\n",
        "    return total_good_classifications / dataset_size # divide by total size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the fcnn class\n",
        "class FCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lin1 = nn.Linear(784, teacher_hidden_size_HP)\n",
        "        self.lin2 = nn.Linear(teacher_hidden_size_HP, teacher_hidden_size_HP)\n",
        "        self.lin3 = nn.Linear(teacher_hidden_size_HP, teacher_hidden_size_HP)\n",
        "        self.lin4 = nn.Linear(teacher_hidden_size_HP, teacher_hidden_size_HP)\n",
        "        self.lin5 = nn.Linear(teacher_hidden_size_HP, teacher_hidden_size_HP)\n",
        "        self.lin6 = nn.Linear(teacher_hidden_size_HP, 10)\n",
        "\n",
        "    def middleforward(self, x):\n",
        "        y = self.lin1(x)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin2(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin3(y)\n",
        "        y = self.relu(y)\n",
        "        return y\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.lin1(x)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin2(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin3(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin4(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin5(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.lin6(y)\n",
        "        y = self.relu(y)\n",
        "        return y\n",
        "\n",
        "#Instantiate and put the model on the chosen device\n",
        "teacher = FCNN().to(chosen_device)\n",
        "\n",
        "#Instantiate the optimizer\n",
        "teacher_optimizer = torch.optim.Adam(teacher.parameters())\n",
        "\n",
        "# Create an array to store the val loss\n",
        "# of the student at each epoch\n",
        "teacher_test_loss = []\n",
        "teacher_train_loss = []\n",
        "\n",
        "# Training loop \n",
        "for epoch in range(nepochs_teacher_HP):\n",
        "    for (x_mb, y_mb) in train_iterator:\n",
        "        # Flatten the MNIST images, which come in matrix form\n",
        "        x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "        y_mb = y_mb.to(chosen_device)\n",
        "\n",
        "        teacher_output = teacher(x_mb) \n",
        "\n",
        "        # Backpropagation\n",
        "        loss = teacher_loss_HP(teacher_output, y_mb)\n",
        "        loss.backward()\n",
        "        teacher_optimizer.step()\n",
        "        teacher_optimizer.zero_grad()\n",
        "\n",
        "    #teacher_train_loss.append( round(get_acc(teacher, train_iterator).item(), 3) )\n",
        "    teacher_test_loss.append( round(get_acc(teacher, test_iterator, nb_test_HP).item(), 5) )\n",
        "\n",
        "print(\"Teacher results:\")\n",
        "print(\"Epochs: \", np.arange(1, nepochs_teacher_HP+1).tolist())\n",
        "print(\"Train loss: \", teacher_train_loss)\n",
        "print(\"Test loss: \", teacher_test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-h7WlXnnmJV",
        "outputId": "5c7b2892-a49d-4190-a40f-44142965e6b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher results:\n",
            "Epochs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
            "Train loss:  []\n",
            "Test loss:  [0.205, 0.26762, 0.34435, 0.45653, 0.50299, 0.48948, 0.56028, 0.54766, 0.59326, 0.6071, 0.59979, 0.62026, 0.60682, 0.63139, 0.62401, 0.62037, 0.62014, 0.60877, 0.62892, 0.62471, 0.62431, 0.63479, 0.62603, 0.62664, 0.62532]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acY6Jwc7nE3o"
      },
      "source": [
        "# Training the student model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZKdeyavpUl2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7b5a70-987b-4be3-bdac-f42407bb4493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training and testing of model  1  in progress...\n",
            "Train loss:  [0.1    0.1661 0.4116 0.4846 0.5132 0.5831 0.5956 0.5897 0.6158 0.6241\n",
            " 0.6378 0.6462 0.6536 0.6482 0.6559 0.6636 0.663  0.6629 0.6602 0.6693\n",
            " 0.6628 0.6686 0.6674 0.6742 0.6728]\n",
            "Test loss:  [0.1    0.1655 0.4001 0.429  0.4702 0.5413 0.574  0.5739 0.5837 0.5808\n",
            " 0.6085 0.6093 0.6184 0.6305 0.6207 0.6401 0.6455 0.6443 0.6186 0.6367\n",
            " 0.6461 0.645  0.6499 0.6544 0.6391]\n",
            "\n",
            " Training and testing of model  2  in progress...\n",
            "Train loss:  [0.1015 0.119  0.3797 0.4389 0.5075 0.5534 0.5873 0.5957 0.6089 0.628\n",
            " 0.6298 0.6476 0.6561 0.6587 0.655  0.6627 0.6634 0.6624 0.6648 0.6666\n",
            " 0.6652 0.6689 0.6718 0.673  0.6745]\n",
            "Test loss:  [0.1062 0.1071 0.3541 0.3488 0.4645 0.4736 0.5515 0.5282 0.5779 0.5849\n",
            " 0.5936 0.6208 0.6339 0.6379 0.6384 0.6383 0.6412 0.6505 0.6376 0.6445\n",
            " 0.6342 0.6474 0.6454 0.6511 0.6529]\n",
            "\n",
            " Training and testing of model  3  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.3046 0.4579 0.5559 0.5874 0.6209 0.631\n",
            " 0.6378 0.6427 0.6422 0.6587 0.6419 0.6505 0.6566 0.6534 0.6635 0.6707\n",
            " 0.6617 0.6712 0.6717 0.6667 0.6721]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.3019 0.4217 0.519  0.5313 0.5708 0.5982\n",
            " 0.6092 0.6172 0.6186 0.6343 0.6214 0.6118 0.6269 0.6376 0.6339 0.6275\n",
            " 0.6329 0.6412 0.6414 0.6371 0.6498]\n",
            "\n",
            " Training and testing of model  4  in progress...\n",
            "Train loss:  [0.1195 0.1    0.2273 0.4746 0.5111 0.5704 0.6041 0.5993 0.6085 0.6221\n",
            " 0.6218 0.6375 0.641  0.6485 0.6427 0.6498 0.6549 0.6628 0.6645 0.6692\n",
            " 0.6631 0.6715 0.6678 0.6707 0.6717]\n",
            "Test loss:  [0.1229 0.1    0.2009 0.4439 0.4534 0.5044 0.5621 0.5614 0.547  0.5842\n",
            " 0.5933 0.6278 0.6267 0.6333 0.6272 0.6029 0.6535 0.6524 0.6461 0.6522\n",
            " 0.6469 0.6513 0.6311 0.6548 0.6513]\n",
            "\n",
            " Training and testing of model  5  in progress...\n",
            "Train loss:  [0.1    0.1    0.1423 0.2196 0.327  0.4411 0.5246 0.5699 0.5893 0.6066\n",
            " 0.6238 0.6212 0.6427 0.6482 0.6521 0.6557 0.658  0.6571 0.6619 0.6637\n",
            " 0.669  0.6657 0.667  0.6719 0.6673]\n",
            "Test loss:  [0.1    0.1    0.1355 0.2274 0.3111 0.4283 0.4858 0.5354 0.5724 0.5806\n",
            " 0.5842 0.5842 0.6109 0.6297 0.6056 0.6158 0.6194 0.6343 0.643  0.6395\n",
            " 0.6457 0.6429 0.6477 0.6421 0.649 ]\n",
            "\n",
            " Training and testing of model  6  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1    0.1595 0.3257 0.4644 0.5438 0.5746\n",
            " 0.6049 0.6181 0.6252 0.6359 0.6491 0.6479 0.6522 0.6602 0.6566 0.6616\n",
            " 0.6708 0.6728 0.6739 0.6747 0.6692]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1    0.1522 0.3231 0.4432 0.4959 0.5328\n",
            " 0.5714 0.6008 0.6009 0.6108 0.6258 0.623  0.6233 0.6381 0.6234 0.6352\n",
            " 0.6399 0.6429 0.6305 0.6458 0.6485]\n",
            "\n",
            " Training and testing of model  7  in progress...\n",
            "Train loss:  [0.1    0.179  0.1327 0.2163 0.4592 0.5162 0.556  0.5729 0.6    0.6055\n",
            " 0.6196 0.6303 0.6429 0.6422 0.6457 0.6476 0.6563 0.661  0.6665 0.6652\n",
            " 0.6657 0.6654 0.6686 0.6691 0.6689]\n",
            "Test loss:  [0.1    0.1663 0.1133 0.2179 0.4423 0.4538 0.5001 0.5469 0.5791 0.5861\n",
            " 0.5899 0.5923 0.6293 0.6281 0.6359 0.6375 0.637  0.6496 0.6504 0.6405\n",
            " 0.6482 0.6431 0.6496 0.6539 0.6459]\n",
            "\n",
            " Training and testing of model  8  in progress...\n",
            "Train loss:  [0.1    0.1846 0.3006 0.4286 0.5019 0.5619 0.5844 0.592  0.6323 0.634\n",
            " 0.641  0.6329 0.6481 0.6518 0.6598 0.6562 0.661  0.6609 0.664  0.6676\n",
            " 0.6712 0.6709 0.6693 0.6754 0.6748]\n",
            "Test loss:  [0.1    0.1802 0.2659 0.3938 0.4698 0.5124 0.5525 0.5731 0.59   0.6076\n",
            " 0.6077 0.6127 0.628  0.6219 0.6317 0.6212 0.624  0.6332 0.6347 0.6396\n",
            " 0.6489 0.6328 0.6385 0.6488 0.6443]\n",
            "\n",
            " Training and testing of model  9  in progress...\n",
            "Train loss:  [0.1    0.1    0.2044 0.426  0.4676 0.5155 0.5618 0.5804 0.6021 0.605\n",
            " 0.6202 0.6298 0.6191 0.6387 0.6514 0.6569 0.6453 0.6601 0.6526 0.66\n",
            " 0.6615 0.6649 0.6664 0.6672 0.6696]\n",
            "Test loss:  [0.1    0.1    0.2117 0.3984 0.4414 0.4785 0.5283 0.534  0.5718 0.5607\n",
            " 0.5943 0.6081 0.5689 0.6124 0.6299 0.6304 0.6213 0.64   0.6249 0.635\n",
            " 0.6484 0.6542 0.6424 0.6515 0.6562]\n",
            "\n",
            " Training and testing of model  10  in progress...\n",
            "Train loss:  [0.1    0.1    0.3703 0.4598 0.5172 0.5549 0.5773 0.5949 0.61   0.6256\n",
            " 0.6309 0.6408 0.6565 0.6558 0.6584 0.6598 0.6585 0.669  0.6675 0.6691\n",
            " 0.6668 0.6658 0.6692 0.6731 0.6771]\n",
            "Test loss:  [0.1    0.1    0.3372 0.4115 0.4698 0.4957 0.5341 0.5466 0.5737 0.5997\n",
            " 0.6007 0.6187 0.6397 0.6247 0.633  0.6389 0.6448 0.639  0.6571 0.6357\n",
            " 0.6373 0.6481 0.6327 0.6536 0.6592]\n",
            "\n",
            " Training and testing of model  11  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1    0.1255 0.3191 0.4927 0.5182 0.5579\n",
            " 0.5821 0.5957 0.6107 0.6256 0.6306 0.6406 0.632  0.6498 0.6498 0.656\n",
            " 0.6488 0.6589 0.6684 0.671  0.6616]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1    0.1082 0.2957 0.4481 0.495  0.5391\n",
            " 0.5485 0.5488 0.5808 0.584  0.5753 0.6148 0.5954 0.6164 0.6236 0.6147\n",
            " 0.6308 0.6316 0.6611 0.6566 0.6531]\n",
            "\n",
            " Training and testing of model  12  in progress...\n",
            "Train loss:  [0.1    0.1    0.1413 0.2998 0.4323 0.5031 0.557  0.5991 0.6006 0.6116\n",
            " 0.6296 0.6448 0.6398 0.6448 0.6524 0.6542 0.6568 0.662  0.6641 0.6584\n",
            " 0.6683 0.6608 0.6664 0.6671 0.6701]\n",
            "Test loss:  [0.1    0.1    0.1292 0.2466 0.4121 0.469  0.525  0.5469 0.5784 0.5904\n",
            " 0.5981 0.6286 0.6232 0.615  0.6399 0.635  0.6397 0.644  0.6515 0.6438\n",
            " 0.6486 0.6559 0.6474 0.6473 0.643 ]\n",
            "\n",
            " Training and testing of model  13  in progress...\n",
            "Train loss:  [0.1446 0.2413 0.2513 0.4483 0.5505 0.5751 0.5773 0.6071 0.6136 0.6184\n",
            " 0.6301 0.632  0.6483 0.6345 0.6593 0.6514 0.6569 0.6666 0.6621 0.6659\n",
            " 0.6657 0.6657 0.6694 0.6711 0.6701]\n",
            "Test loss:  [0.1409 0.2301 0.2045 0.4209 0.5075 0.5253 0.5412 0.5677 0.5737 0.5848\n",
            " 0.6025 0.5919 0.6306 0.6087 0.6363 0.6352 0.6278 0.6566 0.6476 0.6585\n",
            " 0.6336 0.6544 0.6466 0.6603 0.6524]\n",
            "\n",
            " Training and testing of model  14  in progress...\n",
            "Train loss:  [0.176  0.3657 0.3887 0.4753 0.5358 0.5551 0.5781 0.6008 0.6218 0.6308\n",
            " 0.6421 0.6488 0.6419 0.6526 0.6548 0.6607 0.647  0.6552 0.6658 0.661\n",
            " 0.6659 0.6601 0.6721 0.672  0.6672]\n",
            "Test loss:  [0.1845 0.339  0.3851 0.431  0.4932 0.4892 0.5466 0.5532 0.5988 0.6017\n",
            " 0.6168 0.6159 0.6088 0.6126 0.6263 0.6478 0.6204 0.6241 0.6566 0.6349\n",
            " 0.6452 0.6346 0.6438 0.6422 0.6398]\n",
            "\n",
            " Training and testing of model  15  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1019 0.3933 0.3465 0.5166 0.573  0.5927\n",
            " 0.6091 0.5922 0.6236 0.633  0.6329 0.6439 0.6505 0.6565 0.6591 0.6574\n",
            " 0.6608 0.6593 0.6649 0.6643 0.6637]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1036 0.3868 0.3089 0.4838 0.5229 0.5588\n",
            " 0.5656 0.5505 0.5771 0.6108 0.6073 0.6257 0.6311 0.6258 0.6381 0.6365\n",
            " 0.6293 0.6376 0.6368 0.636  0.6356]\n",
            "\n",
            " Training and testing of model  16  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.2122 0.4634 0.495  0.5793 0.583  0.5575 0.5955\n",
            " 0.6099 0.6222 0.6403 0.6429 0.6504 0.6501 0.6481 0.656  0.6538 0.6611\n",
            " 0.6644 0.6593 0.6603 0.6666 0.675 ]\n",
            "Test loss:  [0.1    0.1    0.1    0.2275 0.4472 0.4482 0.5285 0.5245 0.5313 0.5439\n",
            " 0.5919 0.5726 0.6042 0.5983 0.6253 0.6239 0.6203 0.6316 0.6316 0.6343\n",
            " 0.6454 0.6292 0.6533 0.6312 0.6456]\n",
            "\n",
            " Training and testing of model  17  in progress...\n",
            "Train loss:  [0.1    0.1    0.2646 0.3636 0.4784 0.495  0.5707 0.5674 0.5744 0.5955\n",
            " 0.616  0.6245 0.6191 0.6376 0.6477 0.6537 0.6586 0.66   0.6531 0.6567\n",
            " 0.6657 0.6592 0.6626 0.6644 0.6661]\n",
            "Test loss:  [0.1    0.1    0.2638 0.356  0.4279 0.4618 0.5286 0.4992 0.5328 0.5554\n",
            " 0.57   0.5749 0.579  0.6005 0.6256 0.6235 0.6242 0.6405 0.6322 0.6364\n",
            " 0.6392 0.6311 0.6369 0.6357 0.6494]\n",
            "\n",
            " Training and testing of model  18  in progress...\n",
            "Train loss:  [0.1    0.1421 0.4078 0.4807 0.5227 0.5665 0.5802 0.5955 0.6097 0.6024\n",
            " 0.6189 0.6223 0.6445 0.6511 0.6365 0.6571 0.6634 0.6553 0.6659 0.6675\n",
            " 0.6678 0.6682 0.6756 0.6689 0.6767]\n",
            "Test loss:  [0.1    0.124  0.3683 0.4054 0.464  0.5097 0.5357 0.557  0.5863 0.5574\n",
            " 0.5879 0.5939 0.6131 0.6336 0.6165 0.6334 0.6422 0.6243 0.6425 0.6322\n",
            " 0.6385 0.6501 0.6513 0.6372 0.6582]\n",
            "\n",
            " Training and testing of model  19  in progress...\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1    0.1    0.1099 0.3239 0.4545 0.5681\n",
            " 0.5722 0.6109 0.6283 0.631  0.6476 0.6513 0.6517 0.6565 0.6585 0.6675\n",
            " 0.6627 0.6687 0.6632 0.6717 0.6741]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1    0.1    0.1085 0.2912 0.4238 0.5182\n",
            " 0.4929 0.5687 0.5993 0.5989 0.6248 0.6322 0.6316 0.6445 0.6244 0.6564\n",
            " 0.6348 0.6571 0.6405 0.6445 0.6553]\n",
            "\n",
            " Training and testing of model  20  in progress...\n",
            "Train loss:  [0.1    0.1    0.1778 0.3075 0.4292 0.5038 0.5604 0.584  0.6059 0.6168\n",
            " 0.6361 0.6264 0.646  0.6482 0.653  0.6575 0.6582 0.6589 0.6611 0.6636\n",
            " 0.6632 0.6687 0.6707 0.6677 0.6759]\n",
            "Test loss:  [0.1    0.1    0.1853 0.2905 0.4167 0.4431 0.507  0.5398 0.5505 0.5753\n",
            " 0.6075 0.5744 0.6236 0.6402 0.6353 0.6289 0.6386 0.6389 0.6453 0.6306\n",
            " 0.6439 0.6489 0.6508 0.6473 0.6541]\n"
          ]
        }
      ],
      "source": [
        "def train_a_student():\n",
        "  '''\n",
        "  Trains a student model.\n",
        "  '''\n",
        "  # Initialize the MPS module\n",
        "  student = MPS(\n",
        "      input_dim = 28 ** 2,\n",
        "      output_dim = 10,\n",
        "      bond_dim = bond_dim_HP\n",
        "  ).to(chosen_device)\n",
        "  #student.register_feature_map(feature_map_HP)\n",
        "\n",
        "  # Instantiate the optimizer and softmax\n",
        "  student_optimizer = torch.optim.Adam(\n",
        "      student.parameters(), lr = student_lr_HP, weight_decay = student_reg_HP\n",
        "  )\n",
        "\n",
        "  # Used on the inputs before the loss function\n",
        "  LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  # Create an array to store the val loss\n",
        "  # of the student at each epoch\n",
        "  stud_test_loss = np.array([])\n",
        "  stud_train_loss = np.array([])\n",
        "\n",
        "  # Training loop \n",
        "  for epoch in range(nepochs_student_HP):\n",
        "      for (x_mb, y_mb) in train_iterator:\n",
        "          # Flatten the MNIST images, which come in matrix form\n",
        "          x_mb = x_mb.reshape(-1, 784).to(chosen_device)\n",
        "          y_mb = y_mb.to(chosen_device)\n",
        "\n",
        "          # Add Gaussian noise for the gaussian epochs\n",
        "          if (epoch >= nepochs_student_HP - gauss_epochs_HP):\n",
        "            x_mb = x_mb + torch.randn(size=x_mb.size()).to(chosen_device)\n",
        "\n",
        "          student_output = LogSoftmax( student(x_mb) )\n",
        "          teacher_output = LogSoftmax( teacher(x_mb) )\n",
        "\n",
        "          # Backpropagation\n",
        "          loss = student_loss_HP(student_output, teacher_output)\n",
        "          loss.backward()\n",
        "          student_optimizer.step()\n",
        "          student_optimizer.zero_grad()\n",
        "\n",
        "      # Get accuracy over all test and training data for current epoch\n",
        "      train_current_accuracy = round( get_acc(student, train_iterator, nb_train_HP).item(), 4)\n",
        "      test_current_accuracy = round( get_acc(student, test_iterator, nb_test_HP).item(), 4)\n",
        "      stud_train_loss = np.append(stud_train_loss, train_current_accuracy)\n",
        "      stud_test_loss = np.append(stud_test_loss, test_current_accuracy)\n",
        "  return(stud_train_loss, stud_test_loss)\n",
        "\n",
        "# Repeat the training process in order to get the variance\n",
        "global_stud_test_loss = np.array([])\n",
        "global_stud_train_loss = np.array([])\n",
        "for i in range(20):\n",
        "  print(\"\\n Training and testing of model \", i+1, \" in progress...\")\n",
        "  (stud_train_loss, stud_test_loss) = train_a_student()\n",
        "  print(\"Train loss: \", stud_train_loss)\n",
        "  print(\"Test loss: \", stud_test_loss)\n",
        "  if (i == 0):\n",
        "    global_stud_train_loss = stud_train_loss\n",
        "    global_stud_test_loss = stud_test_loss\n",
        "  else:\n",
        "    global_stud_train_loss = np.vstack((global_stud_train_loss, stud_train_loss))\n",
        "    global_stud_test_loss = np.vstack((global_stud_test_loss, stud_test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the final results and save them for the report.\n",
        "print(\"Final results\")\n",
        "print(\"Train loss: \", global_stud_train_loss)\n",
        "print(\"Test loss: \", global_stud_test_loss)\n",
        "\n",
        "np.save('20x_fc_to_mps_trainloss_40bd', stud_train_loss)\n",
        "np.save('20x_fc_to_mps_testloss_40bd', stud_test_loss)\n"
      ],
      "metadata": {
        "id": "RQr9AIrMzJ1A",
        "outputId": "d17dbf17-4ff3-4016-e900-50e367d057d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results\n",
            "Train loss:  [[0.1    0.1661 0.4116 0.4846 0.5132 0.5831 0.5956 0.5897 0.6158 0.6241\n",
            "  0.6378 0.6462 0.6536 0.6482 0.6559 0.6636 0.663  0.6629 0.6602 0.6693\n",
            "  0.6628 0.6686 0.6674 0.6742 0.6728]\n",
            " [0.1015 0.119  0.3797 0.4389 0.5075 0.5534 0.5873 0.5957 0.6089 0.628\n",
            "  0.6298 0.6476 0.6561 0.6587 0.655  0.6627 0.6634 0.6624 0.6648 0.6666\n",
            "  0.6652 0.6689 0.6718 0.673  0.6745]\n",
            " [0.1    0.1    0.1    0.1    0.3046 0.4579 0.5559 0.5874 0.6209 0.631\n",
            "  0.6378 0.6427 0.6422 0.6587 0.6419 0.6505 0.6566 0.6534 0.6635 0.6707\n",
            "  0.6617 0.6712 0.6717 0.6667 0.6721]\n",
            " [0.1195 0.1    0.2273 0.4746 0.5111 0.5704 0.6041 0.5993 0.6085 0.6221\n",
            "  0.6218 0.6375 0.641  0.6485 0.6427 0.6498 0.6549 0.6628 0.6645 0.6692\n",
            "  0.6631 0.6715 0.6678 0.6707 0.6717]\n",
            " [0.1    0.1    0.1423 0.2196 0.327  0.4411 0.5246 0.5699 0.5893 0.6066\n",
            "  0.6238 0.6212 0.6427 0.6482 0.6521 0.6557 0.658  0.6571 0.6619 0.6637\n",
            "  0.669  0.6657 0.667  0.6719 0.6673]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1595 0.3257 0.4644 0.5438 0.5746\n",
            "  0.6049 0.6181 0.6252 0.6359 0.6491 0.6479 0.6522 0.6602 0.6566 0.6616\n",
            "  0.6708 0.6728 0.6739 0.6747 0.6692]\n",
            " [0.1    0.179  0.1327 0.2163 0.4592 0.5162 0.556  0.5729 0.6    0.6055\n",
            "  0.6196 0.6303 0.6429 0.6422 0.6457 0.6476 0.6563 0.661  0.6665 0.6652\n",
            "  0.6657 0.6654 0.6686 0.6691 0.6689]\n",
            " [0.1    0.1846 0.3006 0.4286 0.5019 0.5619 0.5844 0.592  0.6323 0.634\n",
            "  0.641  0.6329 0.6481 0.6518 0.6598 0.6562 0.661  0.6609 0.664  0.6676\n",
            "  0.6712 0.6709 0.6693 0.6754 0.6748]\n",
            " [0.1    0.1    0.2044 0.426  0.4676 0.5155 0.5618 0.5804 0.6021 0.605\n",
            "  0.6202 0.6298 0.6191 0.6387 0.6514 0.6569 0.6453 0.6601 0.6526 0.66\n",
            "  0.6615 0.6649 0.6664 0.6672 0.6696]\n",
            " [0.1    0.1    0.3703 0.4598 0.5172 0.5549 0.5773 0.5949 0.61   0.6256\n",
            "  0.6309 0.6408 0.6565 0.6558 0.6584 0.6598 0.6585 0.669  0.6675 0.6691\n",
            "  0.6668 0.6658 0.6692 0.6731 0.6771]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1255 0.3191 0.4927 0.5182 0.5579\n",
            "  0.5821 0.5957 0.6107 0.6256 0.6306 0.6406 0.632  0.6498 0.6498 0.656\n",
            "  0.6488 0.6589 0.6684 0.671  0.6616]\n",
            " [0.1    0.1    0.1413 0.2998 0.4323 0.5031 0.557  0.5991 0.6006 0.6116\n",
            "  0.6296 0.6448 0.6398 0.6448 0.6524 0.6542 0.6568 0.662  0.6641 0.6584\n",
            "  0.6683 0.6608 0.6664 0.6671 0.6701]\n",
            " [0.1446 0.2413 0.2513 0.4483 0.5505 0.5751 0.5773 0.6071 0.6136 0.6184\n",
            "  0.6301 0.632  0.6483 0.6345 0.6593 0.6514 0.6569 0.6666 0.6621 0.6659\n",
            "  0.6657 0.6657 0.6694 0.6711 0.6701]\n",
            " [0.176  0.3657 0.3887 0.4753 0.5358 0.5551 0.5781 0.6008 0.6218 0.6308\n",
            "  0.6421 0.6488 0.6419 0.6526 0.6548 0.6607 0.647  0.6552 0.6658 0.661\n",
            "  0.6659 0.6601 0.6721 0.672  0.6672]\n",
            " [0.1    0.1    0.1    0.1    0.1019 0.3933 0.3465 0.5166 0.573  0.5927\n",
            "  0.6091 0.5922 0.6236 0.633  0.6329 0.6439 0.6505 0.6565 0.6591 0.6574\n",
            "  0.6608 0.6593 0.6649 0.6643 0.6637]\n",
            " [0.1    0.1    0.1    0.2122 0.4634 0.495  0.5793 0.583  0.5575 0.5955\n",
            "  0.6099 0.6222 0.6403 0.6429 0.6504 0.6501 0.6481 0.656  0.6538 0.6611\n",
            "  0.6644 0.6593 0.6603 0.6666 0.675 ]\n",
            " [0.1    0.1    0.2646 0.3636 0.4784 0.495  0.5707 0.5674 0.5744 0.5955\n",
            "  0.616  0.6245 0.6191 0.6376 0.6477 0.6537 0.6586 0.66   0.6531 0.6567\n",
            "  0.6657 0.6592 0.6626 0.6644 0.6661]\n",
            " [0.1    0.1421 0.4078 0.4807 0.5227 0.5665 0.5802 0.5955 0.6097 0.6024\n",
            "  0.6189 0.6223 0.6445 0.6511 0.6365 0.6571 0.6634 0.6553 0.6659 0.6675\n",
            "  0.6678 0.6682 0.6756 0.6689 0.6767]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1    0.1099 0.3239 0.4545 0.5681\n",
            "  0.5722 0.6109 0.6283 0.631  0.6476 0.6513 0.6517 0.6565 0.6585 0.6675\n",
            "  0.6627 0.6687 0.6632 0.6717 0.6741]\n",
            " [0.1    0.1    0.1778 0.3075 0.4292 0.5038 0.5604 0.584  0.6059 0.6168\n",
            "  0.6361 0.6264 0.646  0.6482 0.653  0.6575 0.6582 0.6589 0.6611 0.6636\n",
            "  0.6632 0.6687 0.6707 0.6677 0.6759]]\n",
            "Test loss:  [[0.1    0.1655 0.4001 0.429  0.4702 0.5413 0.574  0.5739 0.5837 0.5808\n",
            "  0.6085 0.6093 0.6184 0.6305 0.6207 0.6401 0.6455 0.6443 0.6186 0.6367\n",
            "  0.6461 0.645  0.6499 0.6544 0.6391]\n",
            " [0.1062 0.1071 0.3541 0.3488 0.4645 0.4736 0.5515 0.5282 0.5779 0.5849\n",
            "  0.5936 0.6208 0.6339 0.6379 0.6384 0.6383 0.6412 0.6505 0.6376 0.6445\n",
            "  0.6342 0.6474 0.6454 0.6511 0.6529]\n",
            " [0.1    0.1    0.1    0.1    0.3019 0.4217 0.519  0.5313 0.5708 0.5982\n",
            "  0.6092 0.6172 0.6186 0.6343 0.6214 0.6118 0.6269 0.6376 0.6339 0.6275\n",
            "  0.6329 0.6412 0.6414 0.6371 0.6498]\n",
            " [0.1229 0.1    0.2009 0.4439 0.4534 0.5044 0.5621 0.5614 0.547  0.5842\n",
            "  0.5933 0.6278 0.6267 0.6333 0.6272 0.6029 0.6535 0.6524 0.6461 0.6522\n",
            "  0.6469 0.6513 0.6311 0.6548 0.6513]\n",
            " [0.1    0.1    0.1355 0.2274 0.3111 0.4283 0.4858 0.5354 0.5724 0.5806\n",
            "  0.5842 0.5842 0.6109 0.6297 0.6056 0.6158 0.6194 0.6343 0.643  0.6395\n",
            "  0.6457 0.6429 0.6477 0.6421 0.649 ]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1522 0.3231 0.4432 0.4959 0.5328\n",
            "  0.5714 0.6008 0.6009 0.6108 0.6258 0.623  0.6233 0.6381 0.6234 0.6352\n",
            "  0.6399 0.6429 0.6305 0.6458 0.6485]\n",
            " [0.1    0.1663 0.1133 0.2179 0.4423 0.4538 0.5001 0.5469 0.5791 0.5861\n",
            "  0.5899 0.5923 0.6293 0.6281 0.6359 0.6375 0.637  0.6496 0.6504 0.6405\n",
            "  0.6482 0.6431 0.6496 0.6539 0.6459]\n",
            " [0.1    0.1802 0.2659 0.3938 0.4698 0.5124 0.5525 0.5731 0.59   0.6076\n",
            "  0.6077 0.6127 0.628  0.6219 0.6317 0.6212 0.624  0.6332 0.6347 0.6396\n",
            "  0.6489 0.6328 0.6385 0.6488 0.6443]\n",
            " [0.1    0.1    0.2117 0.3984 0.4414 0.4785 0.5283 0.534  0.5718 0.5607\n",
            "  0.5943 0.6081 0.5689 0.6124 0.6299 0.6304 0.6213 0.64   0.6249 0.635\n",
            "  0.6484 0.6542 0.6424 0.6515 0.6562]\n",
            " [0.1    0.1    0.3372 0.4115 0.4698 0.4957 0.5341 0.5466 0.5737 0.5997\n",
            "  0.6007 0.6187 0.6397 0.6247 0.633  0.6389 0.6448 0.639  0.6571 0.6357\n",
            "  0.6373 0.6481 0.6327 0.6536 0.6592]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1082 0.2957 0.4481 0.495  0.5391\n",
            "  0.5485 0.5488 0.5808 0.584  0.5753 0.6148 0.5954 0.6164 0.6236 0.6147\n",
            "  0.6308 0.6316 0.6611 0.6566 0.6531]\n",
            " [0.1    0.1    0.1292 0.2466 0.4121 0.469  0.525  0.5469 0.5784 0.5904\n",
            "  0.5981 0.6286 0.6232 0.615  0.6399 0.635  0.6397 0.644  0.6515 0.6438\n",
            "  0.6486 0.6559 0.6474 0.6473 0.643 ]\n",
            " [0.1409 0.2301 0.2045 0.4209 0.5075 0.5253 0.5412 0.5677 0.5737 0.5848\n",
            "  0.6025 0.5919 0.6306 0.6087 0.6363 0.6352 0.6278 0.6566 0.6476 0.6585\n",
            "  0.6336 0.6544 0.6466 0.6603 0.6524]\n",
            " [0.1845 0.339  0.3851 0.431  0.4932 0.4892 0.5466 0.5532 0.5988 0.6017\n",
            "  0.6168 0.6159 0.6088 0.6126 0.6263 0.6478 0.6204 0.6241 0.6566 0.6349\n",
            "  0.6452 0.6346 0.6438 0.6422 0.6398]\n",
            " [0.1    0.1    0.1    0.1    0.1036 0.3868 0.3089 0.4838 0.5229 0.5588\n",
            "  0.5656 0.5505 0.5771 0.6108 0.6073 0.6257 0.6311 0.6258 0.6381 0.6365\n",
            "  0.6293 0.6376 0.6368 0.636  0.6356]\n",
            " [0.1    0.1    0.1    0.2275 0.4472 0.4482 0.5285 0.5245 0.5313 0.5439\n",
            "  0.5919 0.5726 0.6042 0.5983 0.6253 0.6239 0.6203 0.6316 0.6316 0.6343\n",
            "  0.6454 0.6292 0.6533 0.6312 0.6456]\n",
            " [0.1    0.1    0.2638 0.356  0.4279 0.4618 0.5286 0.4992 0.5328 0.5554\n",
            "  0.57   0.5749 0.579  0.6005 0.6256 0.6235 0.6242 0.6405 0.6322 0.6364\n",
            "  0.6392 0.6311 0.6369 0.6357 0.6494]\n",
            " [0.1    0.124  0.3683 0.4054 0.464  0.5097 0.5357 0.557  0.5863 0.5574\n",
            "  0.5879 0.5939 0.6131 0.6336 0.6165 0.6334 0.6422 0.6243 0.6425 0.6322\n",
            "  0.6385 0.6501 0.6513 0.6372 0.6582]\n",
            " [0.1    0.1    0.1    0.1    0.1    0.1    0.1085 0.2912 0.4238 0.5182\n",
            "  0.4929 0.5687 0.5993 0.5989 0.6248 0.6322 0.6316 0.6445 0.6244 0.6564\n",
            "  0.6348 0.6571 0.6405 0.6445 0.6553]\n",
            " [0.1    0.1    0.1853 0.2905 0.4167 0.4431 0.507  0.5398 0.5505 0.5753\n",
            "  0.6075 0.5744 0.6236 0.6402 0.6353 0.6289 0.6386 0.6389 0.6453 0.6306\n",
            "  0.6439 0.6489 0.6508 0.6473 0.6541]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "67bfac4f4aefe1c16f1836a62d55b6e6baa7aba1ac5ce70e93ee8e90eb4f073a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}