{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJYw8wfzOBhA",
        "outputId": "6eb077b7-56f0-4a1c-8607-c0eed48c03ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ScierKnave/TorchMPS.git\n",
            "  Cloning https://github.com/ScierKnave/TorchMPS.git to /tmp/pip-req-build-v1xnsu1d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ScierKnave/TorchMPS.git /tmp/pip-req-build-v1xnsu1d\n",
            "  Resolved https://github.com/ScierKnave/TorchMPS.git to commit f716a08e15d0af50dbfdfc435ab9604e82562ea3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmps==0.1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: opt_einsum>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from torchmps==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.9/dist-packages (from opt_einsum>=3.3.0->torchmps==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmps==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmps==0.1.0) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmps==0.1.0) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmps==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmps==0.1.0) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install \"git+https://github.com/ScierKnave/TorchMPS.git\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.modules.pooling import MaxPool2d\n",
        "%pip install torchmetrics\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "from torchmps import MPS\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4cYNyo4nE3j"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DtL7C860NJlr"
      },
      "outputs": [],
      "source": [
        "# FC to MPS\n",
        "\n",
        "# Hardware hyperparameters\n",
        "chosen_device = torch.device('cuda' \n",
        "if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data hyperparameters\n",
        "nb_train_HP = 2000\n",
        "nb_test_HP = 500\n",
        "batch_sz_HP = 150\n",
        "batch_sz_HP = min(batch_sz_HP, nb_train_HP)\n",
        "nb_classes_HP = 10\n",
        "\n",
        "# Teacher hyperparameters\n",
        "nepochs_teacher_HP = 25\n",
        "teacher_loss_HP = nn.CrossEntropyLoss()\n",
        "teacher_lr_HP = 1e-2\n",
        "teacher_reg_HP = 0.01\n",
        "teacher_hidden_size_HP = 70\n",
        "# Student hyperparameters\n",
        "# MPS parameters\n",
        "bond_dim_HP = 20\n",
        "adaptive_mode_HP = False\n",
        "periodic_bc_HP = False\n",
        "\n",
        "# USING CUSTOM FEATURE MAP WITH FORK?: YES\n",
        "\n",
        "# Training parameters\n",
        "nepochs_student_HP = 25 \n",
        "student_lr_HP = 1e-4\n",
        "student_reg_HP = 0.01\n",
        "student_loss_HP = nn.KLDivLoss(reduction = \"batchmean\", log_target = True)\n",
        "\n",
        "# Gaussian parameters\n",
        "gauss_epochs_HP = 40 # number of epochs with added gaussian noise\n",
        "gn_var_HP = 0.2 #added gaussian noise variance\n",
        "gn_mean_HP = 0 #added gaussian noise mean\n",
        "nepochs_student_HP = 40 + gauss_epochs_HP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvJCqr8QnE3l"
      },
      "source": [
        "# Premilinaries: Importing the data and utils subroutines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "34pmpVgSOBhB"
      },
      "outputs": [],
      "source": [
        "# Import the mnist train dataset\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root = './datasets', train = True,   \n",
        "    transform = transforms.ToTensor(),  download = True )\n",
        "\n",
        "# Create a training batch iterator\n",
        "train_subset = torch.utils.data.SubsetRandomSampler(range(nb_train_HP))\n",
        "train_iterator = torch.utils.data.DataLoader(\n",
        "    dataset = train_set, \n",
        "    sampler = train_subset, batch_size=batch_sz_HP\n",
        "    )\n",
        "\n",
        "# Import the mnist test set\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root = './datasets',\n",
        "    train = False, transform = transforms.ToTensor(),  download = True\n",
        "    )\n",
        "# Create a testing batch iterator\n",
        "test_subset = torch.utils.data.SubsetRandomSampler(range(nb_test_HP))\n",
        "test_iterator = torch.utils.data.DataLoader(\n",
        "    dataset = test_set, \n",
        "    sampler = test_subset, batch_size = batch_sz_HP\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "NlVDkr_-nE3l"
      },
      "outputs": [],
      "source": [
        "# Returns the validation set classification accuracy\n",
        "# of the given input model (this is a higher order function)\n",
        "def get_acc(model, iterator, reshape, dataset_size):\n",
        "    # Get the validation set classification accuracy\n",
        "    total_good_classifications = 0\n",
        "    acc_metric = MulticlassAccuracy(num_classes=nb_classes_HP).to(chosen_device)\n",
        "    for (x_mb, y_mb) in iterator:\n",
        "        if reshape == True: \n",
        "          x_mb = x_mb.reshape(-1, 784)\n",
        "        x_mb = x_mb.to(chosen_device)\n",
        "        y_mb = y_mb.to(chosen_device)\n",
        "        # Add the number of datapoints we classified right to the total\n",
        "        batch_size = x_mb.size()[0]\n",
        "        y_hat = model(x_mb)\n",
        "        batch_good_classifications = batch_size * acc_metric(y_hat, y_mb)\n",
        "        total_good_classifications += batch_good_classifications\n",
        "    return total_good_classifications / dataset_size # divide by total size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the fcnn class\n",
        "import torch.nn as nn\n",
        "class FCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.lin1= nn.Linear(1600, 10)\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = torch.flatten(x, start_dim=1)\n",
        "      x = self.lin1(x)\n",
        "      return x\n",
        "\n",
        "#Instantiate and put the model on the chosen device\n",
        "teacher = FCNN().to(chosen_device)\n",
        "\n",
        "#Instantiate the optimizer\n",
        "teacher_optimizer = torch.optim.Adam(teacher.parameters())\n",
        "\n",
        "# Create an array to store the val loss\n",
        "# of the student at each epoch\n",
        "teacher_test_loss = []\n",
        "teacher_train_loss = []\n",
        "\n",
        "# Training loop \n",
        "for epoch in range(nepochs_teacher_HP):\n",
        "    for (x_mb, y_mb) in train_iterator:\n",
        "        # Flatten the MNIST images, which come in matrix form\n",
        "        x_mb = x_mb.to(chosen_device)\n",
        "        y_mb = y_mb.to(chosen_device)\n",
        "\n",
        "        teacher_output = teacher(x_mb) \n",
        "\n",
        "        # Backpropagation\n",
        "        loss = teacher_loss_HP(teacher_output, y_mb)\n",
        "        loss.backward()\n",
        "        teacher_optimizer.step()\n",
        "        teacher_optimizer.zero_grad()\n",
        "\n",
        "    #teacher_train_loss.append( round(get_acc(teacher, train_iterator).item(), 3) )\n",
        "    teacher_test_loss.append( round(get_acc(teacher, test_iterator, False, nb_test_HP).item(), 5) )\n",
        "\n",
        "print(\"Teacher results:\")\n",
        "print(\"Epochs: \", np.arange(1, nepochs_teacher_HP+1).tolist())\n",
        "print(\"Train loss: \", teacher_train_loss)\n",
        "print(\"Test loss: \", teacher_test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-h7WlXnnmJV",
        "outputId": "a25b4044-5153-49d1-8422-9f7643bbfbc2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher results:\n",
            "Epochs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
            "Train loss:  []\n",
            "Test loss:  [0.62316, 0.81292, 0.88202, 0.90402, 0.92285, 0.92986, 0.94946, 0.95832, 0.95853, 0.95427, 0.95545, 0.97144, 0.96351, 0.97885, 0.96833, 0.97196, 0.96358, 0.96869, 0.96576, 0.97112, 0.96227, 0.96385, 0.95973, 0.96142, 0.96571]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acY6Jwc7nE3o"
      },
      "source": [
        "# Training the student model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ZKdeyavpUl2B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c89aca56-8e42-4ab1-f952-d5f7c09734ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training and testing of model  1  in progress...\n",
            "[0.1]\n",
            "[0.1 0.1]\n",
            "[0.1 0.1 0.1]\n",
            "[0.1 0.1 0.1 0.1]\n",
            "[0.1 0.1 0.1 0.1 0.1]\n",
            "[0.1 0.1 0.1 0.1 0.1 0.1]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256 ]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342 ]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882 ]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906 ]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934 ]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949 ]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387 0.9332]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387 0.9332 0.9296]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387 0.9332 0.9296 0.9407]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387 0.9332 0.9296 0.9407 0.943 ]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387 0.9332 0.9296 0.9407 0.943  0.9351]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387 0.9332 0.9296 0.9407 0.943  0.9351 0.9407]\n",
            "[0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387 0.9332 0.9296 0.9407 0.943  0.9351 0.9407 0.9447]\n",
            "Train loss:  [0.1    0.1    0.1    0.1    0.1    0.1    0.1469 0.1851 0.1897 0.2142\n",
            " 0.2468 0.2335 0.2092 0.2826 0.3296 0.3466 0.3145 0.3426 0.357  0.3405\n",
            " 0.3974 0.3601 0.3213 0.3676 0.3582 0.4283 0.4326 0.3611 0.3713 0.3584\n",
            " 0.4007 0.4056 0.4289 0.3555 0.3543 0.3967 0.4247 0.3824 0.3629 0.4014\n",
            " 0.4008 0.8695 0.8567 0.9202 0.9452 0.9565 0.9563 0.9306 0.9615 0.9673\n",
            " 0.9669 0.9735 0.9767 0.9884 0.9887 0.985  0.9879 0.9864 0.9847 0.9948\n",
            " 0.9974 0.9971 0.9992 0.9995 0.9993 0.9989 0.9973 0.9991 0.9994 0.9968\n",
            " 0.9974 0.9994 0.9996 0.9977 0.999  1.     0.9996 0.9992 1.     1.    ]\n",
            "Test loss:  [0.1    0.1    0.1    0.1    0.1    0.1    0.1445 0.1843 0.1917 0.2178\n",
            " 0.2525 0.2398 0.1978 0.256  0.3138 0.3185 0.2896 0.3154 0.3285 0.342\n",
            " 0.3453 0.3286 0.3034 0.3286 0.3318 0.3663 0.3814 0.3337 0.3473 0.3368\n",
            " 0.3203 0.3771 0.3885 0.3328 0.3459 0.3607 0.3689 0.3394 0.3472 0.3881\n",
            " 0.3725 0.8014 0.7888 0.8829 0.8973 0.9101 0.8943 0.8508 0.9105 0.9106\n",
            " 0.882  0.9227 0.9153 0.906  0.9361 0.9261 0.9178 0.9249 0.9232 0.9431\n",
            " 0.9486 0.9393 0.9349 0.9436 0.9427 0.934  0.9431 0.9273 0.949  0.9368\n",
            " 0.9353 0.9435 0.9387 0.9332 0.9296 0.9407 0.943  0.9351 0.9407 0.9447]\n",
            "\n",
            " Training and testing of model  2  in progress...\n",
            "[0.1]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-4050f2ffaa0f>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Training and testing of model \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" in progress...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;34m(\u001b[0m\u001b[0mstud_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstud_test_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_a_student\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstud_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstud_test_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-4050f2ffaa0f>\u001b[0m in \u001b[0;36mtrain_a_student\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepochs_student_HP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m           \u001b[0;31m# Flatten the MNIST images, which come in matrix form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0mx_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# handle PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I;16\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0m__array_interface__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArrayData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train_a_student():\n",
        "  '''\n",
        "  Trains a student model.\n",
        "  '''\n",
        "  # Initialize the MPS module\n",
        "  student = MPS(\n",
        "      input_dim = 28 ** 2,\n",
        "      output_dim = 10,\n",
        "      bond_dim = bond_dim_HP\n",
        "  ).to(chosen_device)\n",
        "  #student.register_feature_map(feature_map_HP)\n",
        "\n",
        "  # Instantiate the optimizer and softmax\n",
        "  student_optimizer = torch.optim.Adam(\n",
        "      student.parameters(), lr = student_lr_HP, weight_decay = student_reg_HP\n",
        "  )\n",
        "\n",
        "  # Used on the inputs before the loss function\n",
        "  LogSoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  # Create an array to store the val loss\n",
        "  # of the student at each epoch\n",
        "  stud_test_loss = np.array([])\n",
        "  stud_train_loss = np.array([])\n",
        "\n",
        "  # Training loop \n",
        "  for epoch in range(nepochs_student_HP):\n",
        "      for (x_mb, y_mb) in train_iterator:\n",
        "          # Flatten the MNIST images, which come in matrix form\n",
        "          x_mb = x_mb.to(chosen_device)\n",
        "          y_mb = y_mb.to(chosen_device)\n",
        "\n",
        "          # Add Gaussian noise for the gaussian epochs\n",
        "          if (epoch <= (nepochs_student_HP - gauss_epochs_HP)):\n",
        "            x_mb = x_mb + torch.randn(size=x_mb.size()).to(chosen_device)\n",
        "\n",
        "          student_output = LogSoftmax( student(x_mb.reshape(-1, 784)) )\n",
        "          teacher_output = LogSoftmax( teacher(x_mb) )\n",
        "\n",
        "          # Backpropagation\n",
        "          loss = student_loss_HP(student_output, teacher_output)\n",
        "          loss.backward()\n",
        "          student_optimizer.step()\n",
        "          student_optimizer.zero_grad()\n",
        "\n",
        "      # Get accuracy over all test and training data for current epoch\n",
        "      train_current_accuracy = round( get_acc(student, train_iterator, True, nb_train_HP).item(), 4)\n",
        "      test_current_accuracy = round( get_acc(student, test_iterator, True, nb_test_HP).item(), 4)\n",
        "      stud_train_loss = np.append(stud_train_loss, train_current_accuracy)\n",
        "      stud_test_loss = np.append(stud_test_loss, test_current_accuracy)\n",
        "      print(stud_test_loss)\n",
        "  return(stud_train_loss, stud_test_loss)\n",
        "\n",
        "# Repeat the training process in order to get the variance\n",
        "global_stud_test_loss = np.array([])\n",
        "global_stud_train_loss = np.array([])\n",
        "for i in range(20):\n",
        "  print(\"\\n Training and testing of model \", i+1, \" in progress...\")\n",
        "  (stud_train_loss, stud_test_loss) = train_a_student()\n",
        "  print(\"Train loss: \", stud_train_loss)\n",
        "  print(\"Test loss: \", stud_test_loss)\n",
        "  if (i == 0):\n",
        "    global_stud_train_loss = stud_train_loss\n",
        "    global_stud_test_loss = stud_test_loss\n",
        "  else:\n",
        "    global_stud_train_loss = np.vstack((global_stud_train_loss, stud_train_loss))\n",
        "    global_stud_test_loss = np.vstack((global_stud_test_loss, stud_test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the final results and save them for the report.\n",
        "print(\"Final results\")\n",
        "print(\"Train loss: \", global_stud_train_loss)\n",
        "print(\"Test loss: \", global_stud_test_loss)\n",
        "\n",
        "np.save('20x_fc_to_mps_trainloss_40bd', stud_train_loss)\n",
        "np.save('20x_fc_to_mps_testloss_40bd', stud_test_loss)\n"
      ],
      "metadata": {
        "id": "RQr9AIrMzJ1A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "67bfac4f4aefe1c16f1836a62d55b6e6baa7aba1ac5ce70e93ee8e90eb4f073a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}