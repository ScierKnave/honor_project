\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{graphicx}
\usepackage{euler}
\graphicspath{ {./images/} }

\title{Neural network knowledge distillation in tensor networks}
\author{Dereck Piché}
\date{\today}


\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}
void


\section{Approches in knowledge distillation}
\subsection{Response-Based Knowledge Distillation}
The first approach is to look exclusively at the outputs
of the student and teachers. Each 

\section{Layer-by-layer approach}
It has been common to each layer of a neural network as a certain abstracted 
representation of the previous information and their for generalisibility. 
Thus, we propose to change the cost, we use a tensor for multilinear mapping 
and train each mapping individualy according to the output of it's paired set of layers. 
\begin{equation*}
\begin{CD}
    x @>>>H_1 @>>>H_2 @>>>(\dots)@>g>> \hat{y}
\end{CD}
\end{equation*}
Each hidden layer is of the form;
\begin{equation*}
    a^l = \sigma^l \bigl( W^l a^{l-1} \bigr)
\end{equation*}
In tensor layer form, it will be defined as 
\begin{equation*}
    a^l = T^l \cdot \Phi \bigl( a^{l-1} \bigr)
\end{equation*}b
The main difference is that in the tensor approach, the "heavy" part is done by the non-linear transformation while a little work is done with the linear mapping. The opposite is true with neural networks. Here, $\Phi(X)$ (X begin the input vector) is a tensor product of several identical non-linear mappings of each element $x_i$. Thus, we have 
\begin{equation*}
    \Phi(X) = \phi(x_1)\phi(x_2)\dots\phi(x_n)
\end{equation*}
Were each $\phi : \mathbb{R} \rightarrow \mathbb{R}^d$, and each $d > 1$. Thus, our $\Phi(X) : \mathbb{R}^n \rightarrow \mathbb{R}^{ (d \times)^{n-1}d} $. In other words, our $\Phi$ returns a tensor of order $n$, where each indices run from $1$ to $d$.
\section{Tensor networks}

\subsection{The \textit{Matrix Product State} Tensor Network}
\subsection{Expressivity of MPS combinations with transformation $[1,x]^t$}
Soit $f(x) R^d \mapsto R^{2^d}$ une fonction qui prend un 
vecteur de variables et qui renvoit un tenseur de dimensions quelconques
dont les éléments contiennent les bases de la fonction multilinéaire des
variables du vecteur $x$. 

The vector given by $f(x)$ will return a tensor containing
every combination of monomes of degree $z$, where $z$ is the smallest
repeating variable in the input vector.

Soit $m(f, \theta): \ R^{2^d} \mapsto (R^d \mapsto R)$ une fonction qui effectue une 
combinaison linéaire des éléments de $f$ and returns an \textit{instance} of
a multilinear function. Let this \textit{instance} depend on $\theta$. 

Let $S_v$ be a set of $n_v$ variables.
Let $v*$ be a vector where each variable in the set $S_v$ is repeated $n_v$ times. 

Since the vector given by $f(x)$ will return a tensor containing
every combination of monomes of degree $z$ and $z = n_v$, the elements
of the ouput of $f(x)$ will return the basis for the space 
of $n_v$-degree polynomial over the set of variables $S_v$. 


\end{document}
This is never printed
