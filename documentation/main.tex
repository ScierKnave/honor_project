\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amscd}


\title{Neural network knowledge distillation in tensor networks}
\author{Dereck PichÃ©}


\date{\today}


\begin{document}
\maketitle

\begin{abstract}
This is the paper's abstract \ldots
\end{abstract}

\section{Introduction}
This is time for all good men to come to the aid of their party!

\paragraph{Outline}


\section{Layer-by-layer approach}
It has been common to each layer of a neural network as a certain abstracted representation of the previous information and their for generalisibility. Thus, we propose to change the cost, we use a tensor mapping and train each mapping individualy.

\begin{equation*}
\begin{CD}
    x @>>>H_1 @>>>H_2 @>>>(\dots)@>g>> \hat{y}
\end{CD}
\end{equation*}

Each hidden layer is of the form;
\begin{equation*}
    a^l = \sigma^l \bigl( W^l a^{l-1} \bigr)
\end{equation*}

In tensor layer form, it will be defined as 
\begin{equation*}
    a^l = T^l \cdot \Phi \bigl( a^{l-1} \bigr)
\end{equation*}

The main difference is that in the tensor approach, the "heavy" part is done by the non-linear transformation while a little work is done with the linear mapping. The opposite is true with neural networks.

Here, $\Phi(X)$ (X begin the input vector) is a tensor product of several identical non-linear mappings of each element $(x_i)$. Thus, we have 

\begin{equation*}
    \Phi(X) = \phi(x_1)\phi(x_2)\dots\phi(x_d)
\end{equation*}

Were each $\phi : \mathbb{R} \rightarrow \mathbb{R}^d$
and each 




\bibliography{simple}

\end{document}
This is never printed
